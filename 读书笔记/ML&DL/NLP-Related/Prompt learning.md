# Prompt learning

现在prompt基本就火两个，离散化的和连续化的。模板方法，也就是所谓的离散式prompt，无可厚非，没有功劳也有苦劳，工业界也可以从中获益；连续方法，则是在prompt tuning出来之后追平了全数据的性能之后被引爆，号称小参数撬动大结果（我印象里全参数量追平以及超越是要比adapter好一些的），抛弃了解释性换来了性能。

我们来谈谈被追捧的后者。首先真的可以追平吗，我负责任地讲，是的，我们在广泛而非常复杂的任务上依次尝试过都可以追平和超越。但是也有两点代价。

1.极长的训练时间（十倍）。所以后面有什么预训练的prompt（吐槽，PPT作者们麻烦release一下权重），多任务的prompt种种来处理这件事，其实这个结论已经不新了。如果你尝试训练过任何一个连续化的promot方法，你会发现他的斜率远远缓于finetune，而且震荡更为剧烈。

确实省了空间，确实冻结了所连接的大模型，但是因为prompt都加在前面，所以反向传播还是要完整的从最末端传到最顶端，省去仅仅一个不更新的时间，训练还缓慢，所以根本不省时间，反而要多训好几倍的时间。那么问题来了，对工业界，算力比存储更值钱，那么其实根本没有节约，反而是浪费，如果有几亿个用户，那么确实节约了几万个硬盘，但是你要多消耗几亿倍的显卡算力和耗电量，这让工业界怎么使用呢。（但是真的也很希望有人能提出来针对prompt的运算软硬件优化算法来帮助这个bottleneck）

2.相较于预训练过于specific，和其他组件耦合度太高。连续化prompt包括之前的adapter几乎就是数据集处理specific的甚至可以说是huggingface版本specific的。要求必须要是这个预训练模型换了就没用了，必须要是这个数据集换了就没用了，必须要是这个语言换了就没用了，几乎是做完了路就堵好了。和预训练和微调范式比就是一个硬伤，几乎没法scale up和open source。有个东西叫adapterhub已经干了prompt tuning社区下一步可能想干的事，他们在很多个数据集训练了，200多任务，这还只是只用adpater，openprompt要想做好恐怕是还要再乘一下n（连续化prompt方法）×n（语言）…，我都心疼要花的算力。但是即使做出来了，首先工业界没办法拿来你这权重直接部署，即使他抄了你的伪标签也是一样，也没法load chinese BERT那种拿来养活[工程师](https://www.zhihu.com/search?q=工程师&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2304325156})；然后学术界除非要做同一个任务在你这个weight上面再训练，拿来也没太大用，除非拿来一堆weight做cross task的分析之类的。

吐槽了一会，我想表达意思是prompt方法是不是第四个范式，我认为从发论文角度是的，未来的一年发prompt中稿应该还是没啥问题，我们每个人也不是天天都有best paper idea，也是要恰饭。但是从一个更长远的角度看，如果真的想做一个影响力更大的工作，个人认为站在2022年初，prompt最大的使命在于帮助人们找到超大型预训练模型（GPT3，Codex）的上界，进而使得其零样本少样本性能可以接近超越于比他小很多的但是本身并不小模型（T5 base～large，BART）的全数据微调的不错的结果，进而使得NLP的b2b的ai服务（openai的收费GPT3）盈利成为可能，产生下一个增长点为NLP社区续命。并且完成协助预训练模型完成大一统（unifying）的使命。顺便探明当模型足够大之后泛化性所暗示的智能性。

prompt要做，但最好不是提出来雨后春笋的prompt方法，而是做得更加集中于某个依附的模型，比如英文社区就是GPT3 Codex，中文社区就是百度的最大的那个，而且要盯着zero shot和few shot做，整个社区才会从中更大地受益。













做做研究可以，大规模应用很难，问题很多



1. 效果问题。就拿nlu来说，最好的就是p tuning v2，最好也就能干到finetune的水平。但我组同学其实把一系列prompt tuning的方法都跑过，他们的稳定性其实都很一般，甚至很多结果都得靠搜参数才能搜出来。举个小例子，跑ptuning v2，也就是prefix tuning，prompt的长度可能带来几十个点的波动，这还是非常符合adapter思想的以及效果最好的方案了。更不要说谷歌的prompt tuning了，我想很多做过的同学都知道谷歌那个只训embedding的方法有多难调（坑），效果可以说是无比拉胯。试问这么不稳定，效果还没多好，我们这些工业界的人哪敢大规模用？
2. 规模问题。这我觉得是最痛的... 从很多工作不难看出，prompt tuning很多都需要非常大的模型才能拿到很好的效果的，显然很多人根本access不到这么大的模型（再感慨下，我们即使试了超大号t5，谷歌的prompt还是不太能调出来，真难），就算能，好不容易把weight放下你资源不够多训起来照样够呛。像我自己本身就是做大模型的，我有条件训百亿千亿的模型，那轻量级tuning对我的吸引力在哪呢？大模型之所以难搞，无非就是太大占显存。但现在zero2已经广泛普及了，不管是调deepspeed还是各家自研（模仿）也好，zero2就是把梯度和optimizer states拆出去了，这俩大头已经不会占太多显存了，weight摆在那谁也扔不掉，再加上一般下游tune的数据不会特大，这一下看起来我从实用的角度就不是很感冒了...
3. 训练问题 其实怎么设计prompt，多长，怎么初始化，这些对效果的影响都不是小得可以忍忍那种。当然这个领域继续推进可能能找到更好的方法吧，像ppt想通过预训练解决初始化的问题，但总有点大炮打蚊子的感觉...



现在这种soft prompt我觉得和gpt3以及后面ipet等工作玩的hard prompt不是一个性质的东西，它说到底还是adapter，如果说它火了那我们就有必要重新审视下之前adapter相关的工作。当然prompt还是有它正面的一面，它在fewshot上看起来确实效果很突出，是个对整个nlp领域贡献比较大的点吧。但个人始终觉得，prompt tuning这个点太小了，把它捧到第四范式未免有些过誉。















说一些我个人的看法和实践中遇到的问题。

1.参数化的prompt就是另一种形式的adapter，只不过传统意义上的adapter是一个加在ffn后的MLP，prefix tuning则是加到attention上的

2.少样本+普通模型+仅微调少量参数的"prompt"，想媲美或者接近全量参数微调的结果不（太）可能。除非这个prompt经过了大量数据的预训练，本质就是加了更多数据；除非这个（伪）prompt实际加了很多的参数，当然在少样本条件下，很难训好。

3.少样本+超大模型+不同类型的prompt：结果不太可控，hard形式极度依赖如何去设计prompt，soft形式又变成了adapter的样子

4.全量数据+微调少量参数+普通模型，还是有希望媲美全量参数微调的。不过这样本质还是adapter。











以我之前跑的结果来看，目前基于连续Template的Prompt方法，名义上可以降低需要训练的参数量，但因为它的优化参数在输入层，结果实际上几乎没有实现训练阶段计算代价的显著减少——这样一来在很多场景下就本末倒置了。反正你该计算的中间层导数一个都少不了，性能也就那么回事，那我为啥不直接Fine-tune？

我之前在验证搜索场景下的Prompt应用，一开始设定的目标是希望在性能差不多的前提下减少总训练代价（毕竟搜索这边动不动就是上千万甚至上亿的Pairwise样本，当时看点线上的数据训一次12层的Teacher BERT就要四卡跑两周），结果跑了半天发现基本上没卵用……













其他答主从技术或者部署上回答了这个问题，我从工程师使用/期望的角度来说一下想法。

prompt这个东西，最早知道是因为GPT-2带来令人惊喜的few-shot/zero-shot的能力，只要用自然语言去描述你的需求，GPT-2就会给你一个还行的回复，颇有一种自然语言编程的感觉。当时我的感受和一个知友很相似，就是“GPT-2是第一个我会期待它给我什么回复的模型”。

这种能力确实是值得挖掘的，只是我以为大家会研究“怎么让大模型更理解人类的输入”变成了”怎么让人类的输入更方便大模型理解“，也就是重点从我以为的模型侧变成了现在的输入侧。而这点就是我认为prompt learning天生的缺陷。

说得直白点，基于模板的方法尚可接受，但一旦开始参数化在我看来就违背了prompt的初衷了。不然看看现在大多prompt的工作性能是越来越好，但用法上完全就是一种adapter的用法，只是它不放在FFN而已，完全没有了最初prompt那种便捷性和可理解性。

有些工作甚至在prompt上插入多个或者完全采用可学习的token来让模型去学，那它说白了不还是一个adapter吗？和”prompt(提示)“这个词还有啥关系？更进一步去想，如果prompt可以全部学习出来，那是不是意味着可以套娃？

我个人看好用大模型解决few-shot/zero-shot的方向（也包括统一多模态和多任务），但除了Instruction Tuning外不看好其他基于prompt的玩法，大多数都是trick。























