## 模型压缩

压缩/加速一个模型是为了更好地让其进行部署，譬如部署到手机，FPGA，无人机上去。

这个体系一般分： 

- **1）pruning **
- **2）quantization **
- **3） knowledge distillation **
- **4）low-rank decomposition **
- **5）compact architecture design **  

#### 1. 模型剪枝

模型剪枝主要分为结构化剪枝和非结构化剪枝：

- 非结构化剪枝：去除不重要神经元及相应的连接
    - 剪枝后的模型通常很稀疏，原有模型结构改变，称为非结构化剪枝
    - 非结构化剪枝能极大降低模型的参数量和理论计算量
    - 但是现有硬件无法加速稀疏结构，所以运行速度不会提升
- 结构化剪枝：以filter或layer为单位进行剪枝
    - filter/layer剪枝，特征图会变，但模型结构未变，称为结构化剪枝

##### 相关文献的讨论

lucky Lottery：

- 剪枝后使用original initialization再train，效果相似甚至更好
    - original initialization就是“winning ticket”
    - 但必须使用小learning rate
        - learning rate较小，训练完的weights和original initialization 距离较小，相当于信息leak

Rethinking-liu zhuang：

- 剪枝之后随机初始化也一样

结论：

- 对于structured pruning来说，pruned后可train from scratch 达到fine-tuning精度
    - 剪枝算法的价值在于识别有效的结构和执行**隐式架构搜索**；
- 对于非结构化剪枝，剪枝既没用也有用
    - 没用：是否继承参数影响不大，只需要从头开始训练一个稀疏网络
    - 有用：因为非结构化剪枝里面，剪掉的参数实际上是被置成了0 ，但0 也算参数

#### 2. 模型量化

将模型的32位、16位参数等降低至16位、8位、甚至binary的算法，可以大幅降低模型运算量和模型体积

#### 3. 知识蒸馏

知识蒸馏通过使用一个足够冗余的教师模型，来将其知识 “传授”给紧凑的学生模型。在训练时同时使用教师模型的软标签和真实标记的硬标签来共同训练学生模型，从而能够使学生模型达到接近教师模型的性能，也因此能够降低达到目标精度所需的计算量和模型大小。

#### 4. 低秩近似

低秩近似将一个较大的卷积运算或者全连接运算替换成多个低维的运算

常用的低秩近似方法有 CP 分解法，Tucker 分解和奇异值分解。

例如，一个𝑀 × 𝑁的全连接操作若能近似分解为𝑀 × 𝑑和𝑑 × 𝑁（其中𝑑 ≪ 𝑀, 𝑁）那么这一层全连接操作的计算量和参数量将被极大地缩减。

#### 5. 可以不压缩吗？

有没有办法能够**bottom-up**的寻找到给定任务和数据集合适的空间呢？

其实在Incremental learning 或者叫 life long learning 的任务中，有着两种实现途径：

- **Regularization approach**： 对应算法主要就是**蒸馏(distillation），**而网络结构不变
- **Dynamic Architecture**：随着学习任务的不断增加动态的改变网络结构本身