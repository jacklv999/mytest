## 模型压缩

压缩/加速一个模型是为了更好地让其进行部署，譬如部署到手机，FPGA，无人机上去。一个是pretrained model，一个是 the compact version，两者精度一般差不多，后者功耗会低很多，当然要压缩/加速模型。

再者，使用一些加速压缩算法去push一个模型，不但可以得到更快更小的模型，而且精度还可以超出所给的pretrained model（通过distillation）。

更锦上添花的事情：压缩算法还可以将模型push到 low-bit（通过**quantization**实现），实现硬件级别的加速。

最后，模型加速算法体系中的各个分支算法可以叠加使用，譬如你先过一遍pruning再来一次quantization，加速效果嗖嗖的。

忘了说，这个体系一般分： **1）pruning 2）quantization 3） knowledge distillation 4）low-rank decomposition 5）compact architecture design 这几个分支的方法。** 















你问了一个很好的问题，然而大部分提到rethinking的文章也没有好好回答，先说结论吧：

1.假如是每层剪枝比例固定的传统结构化剪枝，没啥必要，确实直接train一个小网络会更好。

2.对于非结构化剪枝，部分有必要，在极大剪枝比例下，继承原来的参数可以获得更好的。

3.对于每层比例不一定的，未知，也许这个就是剪枝的真正价值所在。

在非结构化剪枝上，除了超级大的剪枝比例，以及ImageNet，基本上都被beat。后面还有很多实验，就不一一举例了。







































这里先简单总结一下我们的paper的主要结论，方便后面讨论和LTH的异同：对于structured pruning来说，pruned完之后的model是可以train from scratch 而达到和一般fine-tuning相比同样的精度的，所以对于structured pruning来说更重要的是prune完得到的architecture而非weights。

我们在6种structured pruning方法，不同的网络architecture，CIFAR/ImageNet数据集上实验验证了这个现象均成立。

因此，对于有些structured pruning方法，我们并不需要先训大模型再prune，直接重头训练小模型就可以；另一些structured pruning的方法其实可以看做是architecture search的方法。

我们的paper的主要结论是对structured pruning，然而我们也做了unstructured pruning的实验，发现在CIFAR数据集上，prune完的model还是基本可以from scratch train到和fine-tuning一样好；但在ImageNet数据集上，当prune ratio大到一定程度，from scratch是达不到fine-tuning的performance的。因此，我们paper的主要结论只针对structured pruning。

LTH的pruning只evaluate了unstructured pruning的方法，主要结论是prune完的模型是可以重头训达到competitive的performance，但是前提是必须使用original initialization，也就是“winning ticket”。paper实验说明了使用winning ticket比random initialization要好。主要实验是在一些相对小的model和CIFAR/MNIST上进行的。

好了，两篇paper矛盾的地方来了，那么在unstructured pruning (on CIFAR)重头train小模型时到底需不需要使用winning ticket？使用winning ticket到底相比random initialization有没有提升?

——————learning rate可能是问题的关键——————

在我们后续的unstructured pruning的实验中发现，learning rate是一个很重要的因素。这里的learning rate既指train大model的也指train小model的。我们在CIFAR-10上实验发现，当使用0.01作为starting learning rate时，winning ticket是有帮助的；当使用0.1作为starting learning rate时，是大概没有的。我们的paper中其他实验都使用的是0.1作为starting learning rate，这个learning rate自从ResNet出来后就一直和SGD with momentum一起在classification problem中广为使用，实际上已经是在CIFAR/ImageNet上的默认设置，train过这两个数据集的人应该都懂。我们evaluate的6种之前的pruning的方法也都是使用这个learning rate。使用0.1也确实比0.01的accuracy更高，毕竟是前人调出来的。

——————总结——————

总结一下：

- 在unstructured pruning上，当使用小的learning rate时，winning ticket有帮助；
- 当使用大learning rate时，winning ticket没有帮助；
- 在L1-norm structured pruning上，不管大小learning rate, winning ticket都没有帮助。

至于为什么即使对unstructured pruning, 也只有当小learning rate的时候LTH才成立，我的一个naive的猜想是，当learning rate较小时，最终训完时候的weights和original initialization时候的weights距离较小，所以如果使用original initialization来对小model进行初始化，相当于leak了一些training完后的大model的信息。极端一点的话，甚至可以说，使用了winning ticket的这个小model并不是从scratch训的，而是已经某种程度上based on这个已经train了很久的大model了，所以它能train的相对好。



































Lottery 这篇文章最重要的结论是：

**一个网络，我train到收敛，然后我剪枝，剪枝之后呢，我把那些还没有被剪的参数重新初始化到刚开始初始化的样子，然后再train，发现效果还挺好，有时候甚至更好。**

其实，Rethinking 这篇文章也是差不多的结论，只是有一点不同：

剪枝之后，Lottery 要把未被剪的参数重新变成之前初始化的样子，而 Rethinking 则更简单一些，不需要变成之前初始化的样子，你随便再随机初始化也是一样的。

> 有人说这两篇文章是矛盾的，但是我觉得其实并不矛盾，因为两篇文章其实都在揭示一个重要的点，那就是 pruning 的真正作用也许是在发现更好的（稀疏）结构，而不是在参数上。

后来 Rethinking 的作者 

 也在这个回答里面做了回应，发现是其实这个结论不同其实是因为 learning rate 不同。考虑到 deep learning 本来就是有太多细小而且难以解释和把握的变量，我们这里可以认为：

**是否重新变成之前初始化的样子其实影响不大。**

好了，到这里两篇 paper 基本上是可以达成共识了，我们来总结一下

1. 对于结构化剪枝，剪枝看起来就是真的没啥必要了。因为结构化剪枝，剪掉的filter，那就是真的剪掉了，可以完全不考虑。
2. 对于非结构化剪枝，剪枝既没用也有用。说没用：根据我们前面的分析，继承或者不继承剪枝前或者初始化时的参数其实影响不大，所以我们只需要从头开始训练一个稀疏网络就行了；说有用：因为非结构化剪枝里面，剪掉的参数实际上是被置成了0 。但是，0 也算参数，其实也能 somehow 有点贡献。

如果你 train 两个网络，一个网络是不稀疏的网络，另外一个网络是稀疏网络，让两个网络的 **非零** 参数的数量一样，很明显稀疏网络要好一些。但是这么比较其实不公平，因为后者的参数量其实更多（0也算参数）。其他回答中之所以有一定争议，其实是因为本身这个问题有多种角度去看：

- 继承参数是否有意义？ （这个问题上面两篇文章已经回答了，意义不大）
- 继承结构是否有意义？
- 稀疏化的结构本身是否有意义？















作为常用的模型压缩方法，**网络剪枝（Network Pruning）**被广泛用于降低CNN的模型复杂度与计算量。典型的剪枝算法pipeline通常有三个阶段，首先**训练**一个大型模型，然后进行**剪枝**和最后**微调**。在剪枝过程中，根据一定的标准，对冗余权重进行修剪并保留重要权重，以最大限度地保持精确性。普遍观念认为模型压缩通常能大幅减少参数数量，压缩空间，从而降低计算量。

> Liu, Zhuang, et al. "Rethinking the Value of Network Pruning."*arXiv preprint arXiv:1810.05270*(2018).

在这篇论文里，发现了几个与普遍观念相矛盾的的观察。他们了试验了几种剪枝算法，发现对剪枝后的模型进行fine tuning，只比使用随机初始化权重训练的网络的性能好一点点，甚至性能更差。文章认为自动剪枝算法的价值在于识别有效的结构和执行**隐式架构搜索（implicit architecture search）**，而不是**选择“important”权重**。所以训练一个大型的、**over-parameterized**的模型对于最终得到一个有效的小模型**不是必需的;** 为了得到剪枝后的小模型，求取大模型的“important” weights**不一定有用;** 剪枝得到的结构本身，而不是一组“important” weights，这才是导致最终模型效果提升的原因。总结来说就是剪枝算法可以被视为**“隐式网络结构搜索”（network architecture search）。**

当然问题也是显而易见的，就像 

[@孟让](https://www.zhihu.com/people/497bc66c1e9a6aec2a58a63ec9537200)

 的回答，CNN的模型的代表了其（虽然目前理论上还无法定量的进行描述）。 固定的CNN网络结构的是的，在没训练出模型之前，我们并不知道究竟多大的网络才合适我们给定的任务和数据集，我们并不知道多少的学习容量才是合适的。如果上述文章的结论，即剪枝算法实际上是隐式网络结构搜索成立，从这个角度来说，模型压缩算法是的寻找合适的模型。所以在目前的范式下，模型压缩有着重要的。

有没有办法能够**bottom-up**的寻找到给定任务和数据集合适的空间呢？其实在Incremental learning 或者叫 life long learning 的任务中，有着两种实现途径，一是**Regularization approach**，对应算法主要就是**蒸馏(distillation），**而网络结构不变。另外一种实现途径就是**Dynamic Architecture,** 随着学习任务的不断增加动态的改变网络结构本身。所以我一直在想在一个真正的人工智能系统中，固定不变的网络结构怎么能够不断学习新的任务而不会灾难性遗忘呢？















这里先简单总结一下我们的paper的主要结论，方便后面讨论和LTH的异同：对于structured pruning来说，pruned完之后的model是可以train from scratch (from random initialization) 而达到和一般fine-tuning相比同样（甚至有些情况更好）的精度的，所以对于structured pruning来说更重要的是prune完得到的architecture而非weights。我们在6种structured pruning方法，不同的网络architecture，CIFAR/ImageNet数据集上实验验证了这个现象均成立。因此，对于有些structured pruning方法，我们并不需要先训大模型再prune，直接重头训练小模型就可以；另一些structured pruning的方法其实可以看做是architecture search的方法。

我们的paper的主要结论是对structured pruning，然而我们也做了unstructured pruning (magnitude-based [1]) 的实验，发现在CIFAR数据集上，prune完的model还是基本可以from scratch train到和fine-tuning一样好；但在ImageNet数据集上，当prune ratio大到一定程度，from scratch是达不到fine-tuning的performance的。因此，我们paper的主要结论只针对structured pruning。

LTH的pruning只evaluate了unstructured pruning (magnitude-based [1]) 的方法，主要结论是prune完的模型是可以重头训达到competitive的performance，但是前提是必须使用original initialization，也就是“winning ticket”。paper实验说明了使用winning ticket比random initialization要好。主要实验是在一些相对小的model和CIFAR/MNIST上进行的。

好了，两篇paper矛盾的地方来了，那么在unstructured pruning (on CIFAR)重头train小模型时到底需不需要使用winning ticket？使用winning ticket到底相比random initialization有没有提升?

——————learning rate可能是问题的关键——————

在我们后续的unstructured pruning的实验中发现，learning rate是一个很重要的因素。这里的learning rate既指train大model的也指train小model的。我们在CIFAR-10上实验发现，当使用0.01作为starting learning rate时，winning ticket是有帮助的；当使用0.1作为starting learning rate时，是大概没有的（见下图）。我们的paper中其他实验都使用的是0.1作为starting learning rate，这个learning rate自从ResNet出来后就一直和SGD with momentum一起在classification problem中广为使用，实际上已经是在CIFAR/ImageNet上的默认设置（任何带BN的网络结构），train过这两个数据集的人应该都懂。我们evaluate的6种之前的pruning的方法也都是使用这个learning rate。使用0.1也确实比0.01的accuracy更高，毕竟是前人(可能是ResNet?)调出来的。

可能有人要问了，为什么要相信我们的发现？我宁愿相信[best paper](https://www.zhihu.com/search?q=best+paper&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A678706173})! 哈哈，开个玩笑。很多人没注意到的是，人家LTH paper里虽然主实验用的是小learning rate, 但他们也跑了标准大learning rate的实验，然后发现winning ticket并不比random initialization好 (见下两图，分别是VGG和ResNet-18)！(这里LTH还引用了我们paper) 但是人家把这结果放在最后一个实验section里，不是在他们用来得出结论的前两个section.... 也就是说，作者是知道大learning rate是不work的，只是没有用这个实验来突出强调LTH的局限性。

这里再总结一下paper关于LTH的实验（也包括LTH原paper的实验），就是：在unstructured pruning上，当使用小的learning rate时，winning ticket有帮助；当使用（标准的且accuracy更高的）大learning rate时，winning ticket没有帮助；在L1-norm structured pruning上，不管大小learning rate, winning ticket都没有帮助。更多讨论见我们paper的section 6。这个回答并没有想说LTH的价值不大，只是想指出可能有些其他情况它并不成立。其他paper中，winning ticket不一定有帮助的实验请参考[3], train from scratch不需要使用winning tickets的实验可以参考[4]。

至于为什么即使对unstructured pruning, 也只有当小learning rate的时候LTH才成立，我的一个naive的猜想是（并没有经过实验验证，轻喷），当learning rate较小时，最终训完时候的weights和original initialization时候的weights距离较小（不一定是L2 distance,可能是更抽象的），所以如果使用original initialization来对小model进行初始化，相当于leak了一些training完后的大model的信息。极端一点的话，甚至可以说，使用了winning ticket的这个小model并不是从scratch训的，而是已经某种程度上based on这个已经train了很久的大model了，所以它能train的相对好。当使用大learning rate时，训完的weights和init的相差较远，就不存在这个原因了。















































