# ViT的泛化性能

最近，ViT模型在各类计算机视觉任务中都取得了令人瞩目的成果。然而，人们对它们在面对不同distribution shift（DS）下的泛化能力研究较少。因此，本文提供了关于ViT在OOD数据上泛化能力的全面研究。为了支持系统的研究，本文首先对DS进行基于语义偏离程度的分类，将它们划分为四个概念组：background shift，corruption shift，texture shift和style shift。

进而评估和比较了ViT和卷积神经网络（CNN）模型在不同DS下的泛化性能，并从而得出各模型的归纳偏置（Inductive Bias）上的特点。由此本文得到以下重要观察结果：

- ViT学习到更弱的对于背景和纹理视觉线索的归纳偏置，而装备了更强的对物体形状和结构信息的归纳偏置。这表明ViT相较于CNN，和人类认知特点更为一致。因此，在大多数DS下，ViT的泛化能力优于CNN，即在相同或更少的参数量下，ViT的top-1正确率比对应的CNN领先5%至10%。
- 随着ViT模型尺寸逐渐变大，其逐渐加强了上述归纳偏置特点，同时会逐渐缩小其面对正常分布（in-distribution）数据和OOD数据的泛化性能差距。

然后，为了进一步提高ViT的泛化能力，我们分别设计了结合对抗学习、信息论和自监督学习的三种泛化能力提升的ViT。通过研究这三种类型的泛化增强ViT，我们观察到了ViT模型针对梯度的敏感性，并设计了一个更平滑的学习策略，以实现稳定的训练过程。通过修改的训练方案，我们实现了相较于原始ViT在OOD数据下的泛化性能4%左右的提升。通过将这三种泛化增强的ViT与它们对应的CNN模型进行综合比较，得到以下结论：

- 对于泛化增强的ViT，模型结构越庞大，其对于OOD数据的泛化能力得到的增益更多。
- 与相应的CNN模型相比，泛化增强的ViT对超参数更敏感。希望我们的综合研究能够为设计更一般化的学习架构提供启发。