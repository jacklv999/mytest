<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>变分推断简述</title><style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color:#ffffff; --text-color:#333333; --select-text-bg-color:#B5D6FC; --select-text-font-color:auto; --monospace:"Lucida Console",Consolas,"Courier",monospace; }
html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }
body { margin: 0px; padding: 0px; height: auto; bottom: 0px; top: 0px; left: 0px; right: 0px; font-size: 1rem; line-height: 1.42857; overflow-x: hidden; background: inherit; tab-size: 4; }
iframe { margin: auto; }
a.url { word-break: break-all; }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: var(--select-text-bg-color); color: var(--select-text-font-color); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; overflow-wrap: break-word; position: relative; white-space: normal; overflow-x: visible; padding-top: 40px; }
#write.first-line-indent p { text-indent: 2em; }
#write.first-line-indent li p, #write.first-line-indent p * { text-indent: 0px; }
#write.first-line-indent li { margin-left: 2em; }
.for-image #write { padding-left: 8px; padding-right: 8px; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
.typora-export .footnote-line, .typora-export li, .typora-export p { white-space: pre-wrap; }
@media screen and (max-width: 500px) {
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  #write { padding-left: 20px; padding-right: 20px; }
  .CodeMirror-sizer { margin-left: 0px !important; }
  .CodeMirror-gutters { display: none !important; }
}
#write li > figure:last-child { margin-bottom: 0.5rem; }
#write ol, #write ul { position: relative; }
img { max-width: 100%; vertical-align: middle; }
button, input, select, textarea { color: inherit; font: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
*, ::after, ::before { box-sizing: border-box; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p, #write pre { width: inherit; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p { position: relative; }
p { line-height: inherit; }
h1, h2, h3, h4, h5, h6 { break-after: avoid-page; break-inside: avoid; orphans: 2; }
p { orphans: 4; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
.md-math-block, .md-rawblock, h1, h2, h3, h4, h5, h6, p { margin-top: 1rem; margin-bottom: 1rem; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: 700; font-style: italic; }
a { cursor: pointer; }
sup.md-footnote { padding: 2px 4px; background-color: rgba(238, 238, 238, 0.7); color: rgb(85, 85, 85); border-radius: 4px; cursor: pointer; }
sup.md-footnote a, sup.md-footnote a:hover { color: inherit; text-transform: inherit; text-decoration: inherit; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; }
figure { overflow-x: auto; margin: 1.2em 0px; max-width: calc(100% + 16px); padding: 0px; }
figure > table { margin: 0px !important; }
tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 32px; }
.CodeMirror-gutters { border-right: 0px; background-color: inherit; }
.CodeMirror-linenumber { user-select: none; }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
#write pre { white-space: pre-wrap; }
#write.fences-no-line-wrapping pre { white-space: pre; }
#write pre.ty-contain-cm { white-space: normal; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; background: inherit; position: relative !important; }
.md-diagram-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
#write .md-fences.mock-cm { white-space: pre-wrap; }
.md-fences.md-fences-with-lineno { padding-left: 0px; }
#write.fences-no-line-wrapping .md-fences.mock-cm { white-space: pre; overflow-x: auto; }
.md-fences.mock-cm.md-fences-with-lineno { padding-left: 8px; }
.CodeMirror-line, twitterwidget { break-inside: avoid; }
.footnotes { opacity: 0.8; font-size: 0.9rem; margin-top: 1em; margin-bottom: 1em; }
.footnotes + .footnotes { margin-top: 0px; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; background: 0px 0px; text-decoration: none; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; -webkit-tap-highlight-color: transparent; line-height: normal; font-weight: 400; text-align: left; box-sizing: content-box; direction: ltr; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li .mathjax-block, li p { margin: 0.5rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child, li > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; white-space: normal; }
#write .footnote-line { white-space: pre-wrap; }
@media print {
  body, html { border: 1px solid transparent; height: 99%; break-after: avoid; break-before: avoid; }
  #write { margin-top: 0px; padding-top: 0px; border-color: transparent !important; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  html.blink-to-pdf { font-size: 13px; }
  .typora-export #write { padding-left: 32px; padding-right: 32px; padding-bottom: 0px; break-after: avoid; }
  .typora-export #write::after { height: 0px; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 0.8rem; white-space: pre-wrap; background: rgb(204, 204, 204); display: block; overflow-x: hidden; }
p > .md-image:only-child:not(.md-img-error) img, p > img:only-child { display: block; margin: auto; }
p > .md-image:only-child { display: inline-block; width: 100%; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.md-math-block { width: 100%; }
.md-math-block:not(:empty)::after { display: none; }
[contenteditable="true"]:active, [contenteditable="true"]:focus { outline: 0px; box-shadow: none; }
.md-task-list-item { position: relative; list-style-type: none; }
.task-list-item.md-task-list-item { padding-left: 0px; }
.md-task-list-item > input { position: absolute; top: 0px; left: 0px; margin-left: -1.2em; margin-top: calc(1em - 10px); border: none; }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc-content::after, .md-toc::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); }
.md-toc-item a { text-decoration: none; }
.md-toc-inner:hover { text-decoration: underline; }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: 700; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) {
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
code, pre, samp, tt { font-family: var(--monospace); }
kbd { margin: 0px 0.1em; padding: 0.1em 0.6em; font-size: 0.8em; color: rgb(36, 39, 41); background: rgb(255, 255, 255); border: 1px solid rgb(173, 179, 185); border-radius: 3px; box-shadow: rgba(12, 13, 14, 0.2) 0px 1px 0px, rgb(255, 255, 255) 0px 0px 0px 2px inset; white-space: nowrap; vertical-align: middle; }
.md-comment { color: rgb(162, 127, 3); opacity: 0.8; font-family: var(--monospace); }
code { text-align: left; vertical-align: initial; }
a.md-print-anchor { white-space: pre !important; border-width: initial !important; border-style: none !important; border-color: initial !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: 0px !important; background: 0px 0px !important; text-decoration: initial !important; text-shadow: initial !important; }
.md-inline-math .MathJax_SVG .noError { display: none !important; }
.html-for-mac .inline-math-svg .MathJax_SVG { vertical-align: 0.2px; }
.md-math-block .MathJax_SVG_Display { text-align: center; margin: 0px; position: relative; text-indent: 0px; max-width: none; max-height: none; min-height: 0px; min-width: 100%; width: auto; overflow-y: hidden; display: block !important; }
.MathJax_SVG_Display, .md-inline-math .MathJax_SVG_Display { width: auto; margin: inherit; display: inline-block !important; }
.MathJax_SVG .MJX-monospace { font-family: var(--monospace); }
.MathJax_SVG .MJX-sans-serif { font-family: sans-serif; }
.MathJax_SVG { display: inline; font-style: normal; font-weight: 400; line-height: normal; zoom: 90%; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; }
.MathJax_SVG * { transition: none 0s ease 0s; }
.MathJax_SVG_Display svg { vertical-align: middle !important; margin-bottom: 0px !important; margin-top: 0px !important; }
.os-windows.monocolor-emoji .md-emoji { font-family: "Segoe UI Symbol", sans-serif; }
.md-diagram-panel > svg { max-width: 100%; }
[lang="mermaid"] svg, [lang="flow"] svg { max-width: 100%; height: auto; }
[lang="mermaid"] .node text { font-size: 1rem; }
table tr th { border-bottom: 0px; }
video { max-width: 100%; display: block; margin: 0px auto; }
iframe { max-width: 100%; width: 100%; border: none; }
.highlight td, .highlight tr { border: 0px; }


:root { --side-bar-bg-color: #fafafa; --control-text-color: #777; }
html { font-size: 16px; }
body { font-family: "Open Sans", "Clear Sans", "Helvetica Neue", Helvetica, Arial, sans-serif; color: rgb(51, 51, 51); line-height: 1.6; }
#write { max-width: 860px; margin: 0px auto; padding: 30px 30px 100px; }
#write > ul:first-child, #write > ol:first-child { margin-top: 30px; }
a { color: rgb(65, 131, 196); }
h1, h2, h3, h4, h5, h6 { position: relative; margin-top: 1rem; margin-bottom: 1rem; font-weight: bold; line-height: 1.4; cursor: text; }
h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor { text-decoration: none; }
h1 tt, h1 code { font-size: inherit; }
h2 tt, h2 code { font-size: inherit; }
h3 tt, h3 code { font-size: inherit; }
h4 tt, h4 code { font-size: inherit; }
h5 tt, h5 code { font-size: inherit; }
h6 tt, h6 code { font-size: inherit; }
h1 { padding-bottom: 0.3em; font-size: 2.25em; line-height: 1.2; border-bottom: 1px solid rgb(238, 238, 238); }
h2 { padding-bottom: 0.3em; font-size: 1.75em; line-height: 1.225; border-bottom: 1px solid rgb(238, 238, 238); }
h3 { font-size: 1.5em; line-height: 1.43; }
h4 { font-size: 1.25em; }
h5 { font-size: 1em; }
h6 { font-size: 1em; color: rgb(119, 119, 119); }
p, blockquote, ul, ol, dl, table { margin: 0.8em 0px; }
li > ol, li > ul { margin: 0px; }
hr { height: 2px; padding: 0px; margin: 16px 0px; background-color: rgb(231, 231, 231); border: 0px none; overflow: hidden; box-sizing: content-box; }
li p.first { display: inline-block; }
ul, ol { padding-left: 30px; }
ul:first-child, ol:first-child { margin-top: 0px; }
ul:last-child, ol:last-child { margin-bottom: 0px; }
blockquote { border-left: 4px solid rgb(223, 226, 229); padding: 0px 15px; color: rgb(119, 119, 119); }
blockquote blockquote { padding-right: 0px; }
table { padding: 0px; word-break: initial; }
table tr { border-top: 1px solid rgb(223, 226, 229); margin: 0px; padding: 0px; }
table tr:nth-child(2n), thead { background-color: rgb(248, 248, 248); }
table tr th { font-weight: bold; border-width: 1px 1px 0px; border-top-style: solid; border-right-style: solid; border-left-style: solid; border-top-color: rgb(223, 226, 229); border-right-color: rgb(223, 226, 229); border-left-color: rgb(223, 226, 229); border-image: initial; border-bottom-style: initial; border-bottom-color: initial; text-align: left; margin: 0px; padding: 6px 13px; }
table tr td { border: 1px solid rgb(223, 226, 229); text-align: left; margin: 0px; padding: 6px 13px; }
table tr th:first-child, table tr td:first-child { margin-top: 0px; }
table tr th:last-child, table tr td:last-child { margin-bottom: 0px; }
.CodeMirror-lines { padding-left: 4px; }
.code-tooltip { box-shadow: rgba(0, 28, 36, 0.3) 0px 1px 1px 0px; border-top: 1px solid rgb(238, 242, 242); }
.md-fences, code, tt { border: 1px solid rgb(231, 234, 237); background-color: rgb(248, 248, 248); border-radius: 3px; padding: 2px 4px 0px; font-size: 0.9em; }
code { background-color: rgb(243, 244, 244); padding: 0px 2px; }
.md-fences { margin-bottom: 15px; margin-top: 15px; padding-top: 8px; padding-bottom: 6px; }
.md-task-list-item > input { margin-left: -1.3em; }
@media print {
  html { font-size: 13px; }
  table, pre { break-inside: avoid; }
  pre { overflow-wrap: break-word; }
}
.md-fences { background-color: rgb(248, 248, 248); }
#write pre.md-meta-block { padding: 1rem; font-size: 85%; line-height: 1.45; background-color: rgb(247, 247, 247); border: 0px; border-radius: 3px; color: rgb(119, 119, 119); margin-top: 0px !important; }
.mathjax-block > .code-tooltip { bottom: 0.375rem; }
.md-mathjax-midline { background: rgb(250, 250, 250); }
#write > h3.md-focus::before { left: -1.5625rem; top: 0.375rem; }
#write > h4.md-focus::before { left: -1.5625rem; top: 0.285714rem; }
#write > h5.md-focus::before { left: -1.5625rem; top: 0.285714rem; }
#write > h6.md-focus::before { left: -1.5625rem; top: 0.285714rem; }
.md-image > .md-meta { border-radius: 3px; padding: 2px 0px 0px 4px; font-size: 0.9em; color: inherit; }
.md-tag { color: rgb(167, 167, 167); opacity: 1; }
.md-toc { margin-top: 20px; padding-bottom: 20px; }
.sidebar-tabs { border-bottom: none; }
#typora-quick-open { border: 1px solid rgb(221, 221, 221); background-color: rgb(248, 248, 248); }
#typora-quick-open-item { background-color: rgb(250, 250, 250); border-color: rgb(254, 254, 254) rgb(229, 229, 229) rgb(229, 229, 229) rgb(238, 238, 238); border-style: solid; border-width: 1px; }
.on-focus-mode blockquote { border-left-color: rgba(85, 85, 85, 0.12); }
header, .context-menu, .megamenu-content, footer { font-family: "Segoe UI", Arial, sans-serif; }
.file-node-content:hover .file-node-icon, .file-node-content:hover .file-node-open-state { visibility: visible; }
.mac-seamless-mode #typora-sidebar { background-color: var(--side-bar-bg-color); }
.md-lang { color: rgb(180, 101, 77); }
.html-for-mac .context-menu { --item-hover-bg-color: #E6F0FE; }
#md-notification .btn { border: 0px; }
.dropdown-menu .divider { border-color: rgb(229, 229, 229); }


</style>
</head>
<body class='typora-export os-windows' >
<div  id='write'  class = 'is-node'><h2><a name="%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%EF%BC%88variational-inference%EF%BC%89%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95%E7%AE%80%E8%BF%B0" class="md-header-anchor"></a><span>变分推断（Variational Inference）最新进展简述</span></h2><p><span>原创： Ian Papa </span><a href='javascript:void(0);'><span>PaperWeekly</span></a><span> </span><em><span>6天前</span></em></p><h1><a name="%E5%8A%A8%E6%9C%BA" class="md-header-anchor"></a><span>动机</span></h1><p>&nbsp;</p><p><span>变分推断（Variational Inference, VI）是贝叶斯近似推断方法中的一大类方法，将后验推断问题巧妙地转化为优化问题进行求解，相比另一大类方法马尔可夫链蒙特卡洛方法（Markov Chain Monte Carlo, MCMC），VI 具有更好的收敛性和可扩展性（scalability），更适合求解大规模近似推断问题。</span></p><p>&nbsp;</p><p><span>当前机器学习两大热门研究方向：深度隐变量模型（Deep Latent Variable Model, DLVM）和深度神经网络模型的预测不确定性（Predictive Uncertainty）的计算求解都依赖于 VI，尤其是 Scalable VI。</span></p><p>&nbsp;</p><p><span>其中，DLVM 的一个典型代表是变分自编码器（Variational Autoencoder, VAE），是一种主流的深度生成模型，广泛应用于图像、语音甚至是文本的生成任务上；而预测不确定性的典型代表是贝叶斯神经网络（Bayesian Neural Network, BNN）。</span></p><p>&nbsp;</p><p><span>当前 DNN 的一大缺陷是预测“过于自信”，“不知道自己不知道什么”，对于安全性要求很高的任务来说，难以胜任，而 BNN 不仅给出预测值，而且给出预测的不确定性，从而使得模型“知道自己不知道什么”，BNN 广泛应用于探索与利用（Exploration &amp; Exploitation, EE）问题（比如：主动学习、贝叶斯优化、Bandit 问题）和分布外样本检测问题（比如：异常检测、对抗样本检测）等。</span></p><p>&nbsp;</p><p><strong><span>本文以最经典的 VI 方法 Mean Field VI (MFVI) 为基础，从以下几个角度依次简述 VI 方法的最新进展:</span></strong></p><p>&nbsp;</p><ul><li><p><span>如何更好地度量变分后验分布和真实后验分布之间的差异？ </span></p></li><li><p><span>如何使用更复杂的先验分布来描述参数信息？ </span></p></li><li><p><span>如何使用更复杂的后验分布簇来降低 VI 方法的 bias？ </span></p></li><li><p><span>如何通过随机梯度估计方法来提升 VI 方法的 scalability？</span></p><p>&nbsp;</p><p>&nbsp;</p></li></ul><h1><a name="%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89" class="md-header-anchor"></a><span>问题定义</span></h1><p>&nbsp;</p><p><span>考虑一个一般性的问题， x 是 n 维的观测变量，z 是 m 维的隐变量，贝叶斯模型中需要计算后验分布，如下： </span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uToo3yYO7UjqHicC7JWhm1FlascIBQJNwXnzHSsqiclDicg2gRp71iarl3bA/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>其中，p(z) 是先验分布，p(x|z) 是似然函数， p(x)=∫p(z)p(x|z)，称为 evidence，通常 p(x) 是一个不可积的多重积分，导致后验分布 p(z|x) 无法获得解析解，同时因为 p(x) 只与确定的观测变量有关，在计算时可认为是一个常数。</span></p><p>&nbsp;</p><p><span>VI 假设后验分布用一个变分分布 q(z;θ) 来近似，通过构造如下优化问题：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTj5ib15INLESKEiab195qRjCgTPpuCrON9vfljWo9GCdA6ARSjsf1xYlA/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>来求解使得两个分布距离最小的变分分布参数 θ，从而得到近似后验分布。</span></p><p>&nbsp;</p><p><span>因为真实后验分布是未知的，直接优化公式（2）是一件比较有挑战的事情，VI 巧妙地将其转化为优化 ELBO 的问题。推导过程如下：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTJ3beoQBC428Jc5vY93sL1t8YOxcG4AM9rpV9lnSGCeU0dnOK8sH7Rw/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>等号两边移动一下可得： </span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTLaTszz6hrtROn4x1oKjickS3AXvsQZZ7eWUyOQB98XLogvFLZuq4Deg/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>由 KL Divergence 的定义可知， KL(q(z;θ)||p(z|x;ф))≥0，同时 logp(x;ф) 是个常数，所以求优化问题（2）等价于求如下优化问题： </span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTYoohmNHWwUtpUMA4ibkZYa0ibaFhYsuSPKUYnWcgZP3BsxbNfeDsTiaibg/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>这里的目标函数 ELBO 称为 Evidence Lower BOund (ELBO)，继续推导如下：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uToxojIgBeGmiay4ZOhK6c5SgXVPTZNXoEeT4I7npvrsaEJ4XwuEc19NQ/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>ELBO 的形式推导可由 Jensen 不等式直接推导出，如下：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTebNaEZtjUQfX89vn6icgYictDRibu1DDHY3pdEnlEo36TksQALzRPIx5Q/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>公式（6）和公式（7）是一致的，所以求变分后验分布与真实后验分布 KL Divergence 的最小化等价于求 ELBO 的最大化，而 ELBO 的具体形式如（6）（7）所示，进一步整理可得：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uT4wyhJ5NevDdN7Ge7MecRTpicsJkv0sUFVCrTZKic68OtZxokYe8TnGKw/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>其中第一项可以理解为基于变分后验分布的重建似然函数，第二项是变分后验分布与先验分布的 KL Divergence。</span></p><p>&nbsp;</p><p><span>ELBO 的形式推导是 VI 的基础，也是后续各种VI 方法的前提，大多数 VI 方法都旨在解决高效求解 ELBO 优化的问题。从 ELBO 的形式可以看出，待优化的目标函数是一个函数的期望，如何高效估计出目标的梯度是解决问题的关键。本文将从最经典的 MFVI 讲起，然后依次从几个改进角度来综述 VI 的研究进展。</span></p><p>&nbsp;</p><h1><a name="mean-field-vi-(mfvi)" class="md-header-anchor"></a><span>Mean Field VI (MFVI)</span></h1><p>&nbsp;</p><p><span>MFVI 最早应用于统计物理，假设变分后验分布是一种完全可分解的分布，如下式： </span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTpNOBYIdySVXVSGvhIOgDbH0K94f0ACAXko4oKpMGcQL41sSPzcbdyA/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>将公式（9）代入公式（7），同时只考虑第 j 个分布，可得：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTy2SyxcMlPgQiaHzjpcOmZTNgm0Da8KgbHZNOUPqfmTqe4jLQSsEP4Dw/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>其中，</span><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm432OuArXS3GoqcRs7O7XKlLmya70jGLT0903b9QfeImtpzCJLlLFbeFG7P2pCe9tITEmabVy3Pg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='img' referrerPolicy='no-referrer' /><span>是指除掉第 j 项的所有项，</span><img src='https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgm432OuArXS3GoqcRs7O7XKGZic7FTRgr4EfZnt4CnJMNqn8NNtmZjNoORgYAzmJE0b4ga5KtcwfrA/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='img' referrerPolicy='no-referrer' /><span>是指与第 j 项无关的常数项。 </span></p><p>&nbsp;</p><p><span>公式（10）可以看作是一个负 KL Divergence 项，为使得 ELBO(j) 最大，所以负 KL Divergence 为 0， 可得到：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTuCiawa9to6boDFkVzHYwzj2ib7IUp3cQGHu6l4PGuK9k0NPiav6v5AwmA/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>进一步整理得到：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTdUBaO6LBU877An85QqBjbLqgRSkySicx6T90aia95c2MdR9SazNlVcOw/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>可以利用坐标上升法（Coordinate Ascent, CAVI）来迭代求解该优化问题，具体算法参见下图：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTs42ibUDZvmhfickn9Fo24qzvmCwHEVPxibgoxXxt1UicSg9DgwOicgltkOA/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><h1><a name="%E6%94%B9%E8%BF%9Bmfvi%E7%9A%84%E5%87%A0%E4%B8%AA%E8%A7%92%E5%BA%A6" class="md-header-anchor"></a><span>改进MFVI的几个角度</span></h1><p>&nbsp;</p><p><strong><span>如何更好地度量变分后验分布和真实后验分布之间的差异？</span></strong></p><p>&nbsp;</p><p><span>从公式（2）的目标函数可以看出，VI 将近似推断问题转化为了优化问题，使用的是最基础的分布距离度量方法 KL Divergence，因为 KL Divergence 是一个非对称的度量方法，即 KL(q||p)≠KL(p||q) ， 因此这里存在几个值得深入研究的点。</span></p><p>&nbsp;</p><ul><li><p><span>是否可以用 KL(p||q) 来度量变分后验分布和真实后验分布的距离？</span></p></li><li><p><span>是否可以用其他度量方法来度量两者之间的距离？</span></p><p>&nbsp;</p></li></ul><p><span>本小节中的 Expectation Propagation 旨在回答第一个问题，f-Divergence 和 Stein Disparency 旨在回答第二个问题。</span></p><p>&nbsp;</p><p><strong><span>Expectation Propagation</span></strong></p><p>&nbsp;</p><p><span>从广义上讲，凡是基于一个分布簇进行优化参数来逼近真实后验分布的，都可以归为 VI 方法；从狭义上讲，本文开始定义的问题和思路是最经典的 VI 方法。EP 将公式（2）的目标函数更改如下：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTvyiadgKXFJRDbnCibSMH9eYZiazXLFrQ3G7Ph5MKZ2e1OlbW7kORXcJAw/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /><span> </span></p><p><span>EP 也是一个非常活跃的研究领域，由于本文旨在介绍狭义的 VI 方法，因此对 EP 不作详细介绍，感兴趣的同学可以去看这个页面的内容</span><a href='https://tminka.github.io/papers/ep/roadmap.html' target='_blank' class='url'>https://tminka.github.io/papers/ep/roadmap.html</a><span> 。</span></p><p>&nbsp;</p><p><strong><span>α-Divergence</span></strong></p><p>&nbsp;</p><p><span>KL Divergence 是一种特殊的 α-Divergence，一种常见的 Renyi 定义如下：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTsH1Tb3K0hBwNYulepG6E4gQ47GEYodTgTv4DyPHUzibjDpan7NHR5Kg/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /><span> </span></p><p><span>同时要求，α&gt;0,αneq1,|Dα|&lt;+∞。</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uT4FhbCdVa2xl9qPA4zGwcQeMe9KdCiaFlQ9ApFyYQibJfOcw7mX5JGJow/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p><span>▲ 图：Renyi α-Divergence的几种特殊形式</span></p><p>&nbsp;</p><p><span>而 α Divergence 是一种特殊的 </span><em><span>f</span></em><span> Divergence，形式如下：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTpugUoTdy9uxicSyTK9mkBmShL2jEtnFlia2ah89KbHexcu4ECeUWP7HQ/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /><span> </span></p><p><span>同时要求 </span><em><span>f</span></em><span> 是凸函数，且 </span><em><span>f</span></em><span>(1)=0 。</span></p><p>&nbsp;</p><p><span>除了 Renyi 的定义，还有很多不同的定义，有的定义会恰好可以统一VI 和 EP 两种方法，如下图所示，当 α=0 时，该 Divergence 等价于 KL(q||p)，相当于是 VI 方法；当 α=1 时，该 Divergence 等价于 KL(p||q)，相当于是 EP 方法。</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTNzOXopDewQfEl5VtUBrpOJEsxw4KupMFtHTYvXM7hicnhLISCwhxP8A/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>接着 Renyi 的定义，考虑公式（3）的形式：</span></p><p>&nbsp;</p><p><img src='data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>用 Renyi α-Divergence 来代替公式（16）中的 KL Divergence，定义Variational Renyi Bound（VR Bound，Rényi Divergence Variational Inference），将公式（14）代入可得：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uT8gUnf1CbM5EKS977picicYFcCJ6MmlD23GdJNATArxr7HtzM5ic6GicWmQ/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>从而推导出 VRBound 如下：</span></p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTiaoNWFVJicFLqyYQVDOfxGWokt8ot7wdYeP6gBXPNMc6xhibMCZnOqv4A/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>公式（18）的最后一步时根据 Jeson 不等式得来的，它是公式（7）（8）中ELBO 的一般形式，当 α=0 时，VRBound 将降阶为 ELBO。可根据一般 VI 方法的思路来设计 VRBound 的优化算法，将其应用于各种类型的近似推断任务中。</span></p><p>&nbsp;</p><p><strong><span>Stein Disparency</span></strong></p><p>&nbsp;</p><p><span>Stein Disparency 是近几年比较热门的一种度量两个分布之间距离的方法，定义如下：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTAsndmc4mIxWrZibicUibVNwI6TgDMG7X9pLkE7zH9Gv4oyvale3OVwRibg/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>其中，F 表示一系列光滑的函数。两个分布越相似，Stein Disparency 就越小。</span></p><p>&nbsp;</p><p><span>公式（19）中的右边一项包含了未知的真实后验分布 p(z|x) ，无法计算。如何构造一些合适的 f(z) 可以使得 Ep(z|x)[f(z)]=0 ，从而消除掉未知分布的影响。Stein 的方法给出了一类合适的 f(z) ，如下：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTxBWG3K4d2JZ0XicMdpNJvrIC7w45t8VLPicvFJgxdOR7TABTiaUIU7xvw/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>代入到 Ep(z|x)[f(z)] 中可以得到：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTicdk8YtiaAQGnEBrks1RkljmfhNZKRkTic8FK5twXE9bbnCibcqmdBDiaaQ/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>令其等于 0，得到：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTy8btLhMajM4Ha2lh8DuH2XiaKqAmNlnO6hqOluZ72PicRT94PWbwOoTw/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>当找到合适的 f(z) 使得公式（19）右边的一项为 0，只需要计算前面的一项。近几年有一些工作将此方法应用到了 VI 中，其中具有代表性的是 Stein Variational Gradient Descent 和 Operator VI，前者用了 kernel 的方法来计算，后者用了 GAN 的思路来求解。</span></p><p>&nbsp;</p><p><strong><span>如何使用更复杂的先验分布来描述参数信息？</span></strong></p><p>&nbsp;</p><p><span>先验分布通常是专家经验的一个量化途径，将专家对领域的知识表示为一个先验分布，先验越复杂，表明融入的知识会越多，对后验推断会有较大的影响，为简化计算，先验通常选为高斯分布或者混合高斯分布。近些年的一些研究工作表明，先验分布的复杂度以及超参数的选择对于深度生成模型和贝叶斯神经网络的效果影响很大，本小节简单对先验分布的一些相关工作进行介绍。 </span></p><p>&nbsp;</p><p><span>AISTATS 2018 一篇来自 Max Welling 组的工作，提出了一种新的复杂先验分布 VampPrior（Variational  Mixture of Posteriors Prior），并且在 VAE 上进行了实验测试，相比标准的高斯分布先验和混合高斯分布先验有更好的 Log Likelihood 和表示学习效果。思路如下： </span></p><p>&nbsp;</p><p><span>将公式（8）改写为以下形式：</span></p><p><span> </span><img src='data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>公式（23）中第一项是重建的似然函数，第二项是变分后验分布的熵，第三项是负的变分后验分布和先验分布的交叉熵。</span></p><p>&nbsp;</p><p><span>为了保证 ELBO 最大化，需满足第三项也最大化，问题在于如何找到一个合适的先验分布 ，描述参数为 ，使得其拉格朗日乘子表达式最大：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTCYFiaqTgDds2vlO0hAej4Erv42DyljrI15uk8ZiawJuATic89zpjoXGYg/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>该问题的最优解为：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTUQTGRfsADk2oc3oBukHAOGaicB7NSHeozPVqlgWsKj6WzHIWJDCe8TA/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>如果用公式（25）的分布作为先验会导致计算量非常大，同时带来过拟合的风险。因此，这个工作基于此考虑，用下式来代替（25）：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTylkcwWIDjsWAVylGHyZXzc6qYPicRc05bTm5OsSTQEqLMEKExGv3Hnw/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>这里需要优化的参数</span><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm432OuArXS3GoqcRs7O7XKmSxxJZoFoYAJWicHeWFv7nicOzUEz1QVqxmlmkUbfSZfy6EYjDdlFJ0Q/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='img' referrerPolicy='no-referrer' /><span>。因为此先验分布是与后验分布的混合，所以取名为 Variational Mixture of Posteriors Prior。这里的是一些所谓的伪输入，而非真实的输入，也是需要学习的参数。一方面 VampPrior 是更加复杂的多模态分布，对数据的建模更加准确；另一方面，因为 K&lt;&lt;N，计算量也会相对较少一些。 </span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTn9nFK2djsiaH5JbY9kyltdkFA8d7sMU5fibPO7muTAGsibjlEpRjsFIcw/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>由上图可见，VampPrior 比标准的正态分布和混合高斯分布有着明显的提升，同时因为 Prior 的复杂化和 Posterior 的复杂化是解耦的，如果后验分布采用更加复杂的 Normalized Flow，可能会有更好的效果。</span></p><p>&nbsp;</p><p><span>ICLR 2019 一篇来自 Max Welling 参与的工作 </span><strong><span>Deep Weight Prior</span></strong><span>，提出了一种 implicit prior distribution 来提升 prior 的复杂度。implicit distribution 大概的定义是，无法得到该分布的 pdf，但可以从该分布中进行采样、估计期望和梯度。这个工作的思路如下：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uT8qAAY18nI8H57dLywGiaOpxkyiaC9FF6OD6SFibx9nhH8Zz2wSMaOjrXg/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>其中，p(w) 是一个显式分布密度函数，作为先验分布 p(z) 的先验分布，p(z|w;α) 是一个显式的参数分布密度函数，由参数 α 描述。</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTt2Ostfxx6SbriaOTUntjfdekz7Jbnjj0jP2tn6nFsm0MxSgvLetVzug/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>为统一符号，本文采用了与原文不同的符号表示，但示意图原理一致。从图中可以看出，BNN（Bayesian Neural Network）权重的先验分布可以通过构造一个 VAE 进行学习，而学习的数据则来自相似任务中具有相同网络架构的模型。</span></p><p>&nbsp;</p><p><span>具体地讲，本文在 cifar10 数据集上用两层的 5 * 5 和 7 * 7 卷积核作为网络结构，分别训练了 CNN，从中获取了这两类网络架构的权重值作为数据进行学习。学习得到权重的implicit distribution 之后，作为具有同样结构的Bayesian CNN 的权重的 prior 来应用。</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTFmIxdXxV6EpoF3RN2w9C2UUkPjIiacEshwam4pDjmic4OwCVKsFRvzVA/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>上图中左图为学习到的卷积核，而右图为从隐分布中 sample 出来的卷积核。基于复杂的隐先验分布，这篇工作测试了 BCNN 在小样本数据集上的效果，如下图：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTkYquhF451d2ypkS9EQ7OLfn5f3z4Fb6SWEHThRQNjI2PibhibcXSF23A/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>从上图中可以看出，本文的方法 dwp 比标准正态分布和均匀分布作为先验有更好的效果。</span></p><p>&nbsp;</p><p><strong><span>如何使用更复杂的后验分布簇来降低VI方法的bias？</span></strong></p><p>&nbsp;</p><p><span>变分分布是用来替代真实后验分布的，两者的差异越大，后验推断的系统偏差就会越大。有研究结果表明，变分后验分布簇的选择对变分推断效果的影响非常大。</span></p><p>&nbsp;</p><p><span>经典的 VI，会基于简单的平均场（mean-fifiled）假设，用可分解的高斯分布或者一些简单结构的分布来作为变分分布；现在的 VI，需要解决的是数据规模更大、维度更高的问题，经典 VI 的变分分布难以满足。因此，最近几年有一系列工作来研究如何构造一系列更加复杂且方便计算的复杂后验分布来解决这一问题。</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTh4APqqaVxJmwtytWje71z79wpajB0A46DKPFNnSHdTlzUIru0lf4qg/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>上图中，最右边表示分布簇表达能力最弱的 MF，最左边表示表达能力最强的真实后验，中间方法都是对 MFVI 的改进，通过复杂化后验分布来降低 bias。</span></p><p>&nbsp;</p><p><strong><span>Copula*</span></strong><span>*方法**</span></p><p>&nbsp;</p><p><span>大多数的 VI 方法都基于 Mean-Field 的思路，假设变分后验分布中隐变量之间相互独立，这个假设太强，对结果有一定的影响。</span></p><p>&nbsp;</p><p><span>NIPS 2015 一篇 David M. Blei 组的工作 </span><strong><span>Copula Variational Inference</span></strong><span> 尝试用统计学的经典方法 Copula 来解决 MF 中隐变量的独立假设问题。这篇工作的动机非常简单，就是找到一种既考虑隐变量之间的关联性同时也容易进行大规模计算的方法。思路如下：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uT0FcZjAd16Hn1hBibwFlUxwQ2iaGNWMavFD724gVAaibYqBoSX0SKMXYuQ/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>其中公式中的前半部分是 Mean-Field，而后半部分正是所谓的 Copula。</span></p><p>&nbsp;</p><p><span>将公式（28）代入到公式（8）得到 Copula VI 的 EBLO，剩下的工作就是推导 ELBO 的梯度估计式，利用随机优化算法更新参数，不同于一般的 VI，Copula VI 有两种参数，一种是描述变分分布的参数，另一种是描述 Copula 的参数，在训练时，固定其中一种来训练另外一种。梯度估计的公式推导在下一小节会有详细介绍，这里不再赘述。</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uT4ykOre0oqVh3qAZvqo3D3wrvBgDXrxbBl3OrDPziaib43lx3Z9bOzLEg/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>从上图中可以看出 Copula VI 比 MFVI 离真实后验分布更近，bias 更小。Copula 是统计学中的经典方法，对此感兴趣的读者可以去找相关资料进行学习。</span></p><p>&nbsp;</p><p><strong><span>辅助变量法</span></strong></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTuTWicqxFAYfaaL4HBcGfkae6DluiaOZxFrkhCE0rgEBK7ibz2Po2P5dpQ/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>辅助变量法的思路比较简单，它认为隐变量 z 背后还有隐变量 w，是一种层次化建模的思想。即：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTM1FXB9R17EJf2X6UpO5Frws5slW7O1fFdR84CKQfyLhpZYjpsDvwibw/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>图中 r(w|x,z) 正是所谓的辅助变量。将公式（29）代入到公式（8）中可以得到该方法的 ELBO，推导过程类似，这里不再赘述。这种引入辅助变量的方法，其实也是令变分后验分布成为一种表达能力更强的隐分布。</span></p><p>&nbsp;</p><p><strong><span>Normalized Flow</span></strong><span> </span><strong><span>归一化流</span></strong></p><p>&nbsp;</p><p><span>实际应用中的真实后验分布往往是非常复杂的多模态分布，如何构造出一个复杂的分布簇来逼近多模态分布十分重要。本节介绍的归一化流正是解决这个问题的合适方法。归一化流是一系列分布变换操作，可将简单的高斯分布变换成任意形状的分布。</span></p><p>&nbsp;</p><p><span>归一化流的基础是随机变量分布的变换：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uT0054gWwibCb3d6SS6NRsvHy60FWgHLnMMPgBrlgIg58K5eicQXhRqQ0w/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>其中，p(y) 是 y 的分布，p(x) 是 x 的分布，这里 y=f(x)，J 是指雅可比矩阵，即多元函数一阶导数矩阵。</span></p><p>&nbsp;</p><p><span>归一化流以及其基础版 pathwise derivative（下一节介绍），核心都在解决一个问题，能否找到一个合适的双射（one-to-one mapping）保证正向映射过程可以很容易 sampling，同时容易计算其雅可比行列式；反向过程，容易计算 inverse function。如果可以解决上述两个问题，就可以将非常简单的分布，比如：均匀分布和高斯分布，通过一系列的变换（Flow）生成出复杂的分布和预期的分布，如下图。</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTGicHz9ib0jDHf06zybQwvkM0fy5HaG6PEMgAMdZpaG79xemfINuxSPJg/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>如果初始分布经过 K 次变换如下：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTfXk4AEzQkO6JXEicAlysNx7HYp2vzlky2cdAWmvjP9PLoCWQ7tC7HtQ/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>其概率密度函数 pdf 如下：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTic2kz42KWzNo5DzJNE0hvOiaf0MQcsIuxgNSQqOysKiaKIdvr1mLLua0Q/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>可以进行如此推导的依据是 law of the unconscious statistician (LOTUS) ，在 pathwise derivative 一节也会提到。</span></p><p>&nbsp;</p><p><span>Rezende 和 Mohamed 在 2015 年的 ICML 上提出了用归一化流作为变分后验分布，并给出了两种 baseline 分布变换，一种是 Planar （一种线性变换）：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uT1Y6jIicmde1mzOHDUubeWxSHd02qWwsYmf7mwzvkRTLqtb2JaHXf5ZQ/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>这里 h 是一个光滑的非线性函数，先求公式（32）中的行列式：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTtpnqGMJlICSEBxeic9HYwvCHnwiadEkHmJGnzr6QE9ib2KJNJMkDK3FFQ/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>其中</span><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgm432OuArXS3GoqcRs7O7XKKFXOCF1EvlTtrZ2icRg2vJvyV2afapsmZVS2ibicRxlAQjRicoStYF6Gibg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='img' referrerPolicy='no-referrer' /><span>，代入到公式（32）可得：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uT9nsObSIfCY1fuMH5VQLJ7PWP1N628IbFJkfLSEMwnr0XuK49OyXUrg/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>一种是 Radial（一种极性变换）：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTefJQf1iaEgf6krIYMlfJODxjCM2u2o4IAicibgPDucIVdx0ZVdcrJiacmw/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>以 Planar 为例，将归一化流代入到EBLO 中可以得到：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTKVaHRbPxibpJWPRBa3jaiaZqia93licFAFnbDTb3Dg3bw8MXnRI1HMBDwA/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTJiaH4GIjLxVE1teKEzlclvRAmicDoSgp3e8yltWXY0Lcibjtk8uibmpgrQ/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>基于归一化流的 VI 方法在求解时和一般的 VI 没太多区别，这里不再赘述。上图是基于归一化流做的一个分布逼近实验，从结果中可以看出通过 32 次分布变换之后，可以准确地逼近左图中给出的复杂分布。</span></p><p>&nbsp;</p><p><span>除了文章中介绍的归一化流方法，最近几年学术界提出了很多种 Flow 的方法，比如：NICE、Masked Autoregressive Flow（MAF）、Inverse Autoregressive Flow（IAF）等。感兴趣的同学可以去看Stanford CS236 Deep Generative Models Course。</span></p><p>&nbsp;</p><p><strong><span>如何通过随机梯度估计方法来提升VI方法scalability？</span></strong></p><p>&nbsp;</p><p><span>从变分推断的优化目标函数 ELBO 中可以看到，需要优化的是一个函数的期望，而非确定性的函数。相比于其他优化算法，基于梯度的随机优化算法在解决大规模数据、高维度问题中有着巨大的优势，因此，如何准确估计出函数期望的导数是核心问题。机器学习中，常用的随机梯度估计方法包括：score Function 和 pathwise derivative。</span></p><p>&nbsp;</p><p><strong><span>Score Function (SF)</span></strong></p><p>&nbsp;</p><p><span>所谓的 score function 是 ，score function 的期望为 0，证明如下：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTPQYzEvL3g7EUm3J5fwLAu8RT05dlTjYiaQXiaj7kxvz9SY9OYsibEaibEQ/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>这样会带来非常多的便利，比如：一种降低估计方差的思路，将代价函数 f(x) 改造为 f(x)-b ，其中 b 是所谓的 baseline。因为 score function 的期望为 0，所以：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTTITicM2mAHAibx2tEqbD4NgT4c0nQODoJXP0W24AgRHYT2mPET1kjOvg/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='img' referrerPolicy='no-referrer' /><span> </span></p><p><span>ELBO 推导出的优化问题如下：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTApAQ7ia6pEK2ibEnMVR5ozBCjdnn9uRUNERtJWeBJTH2zGWDGZHqHW9Q/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>其中，x 是观测变量，z 是隐变量，q(z) 是变分分布，λ 是变分分布的参数。</span></p><p>&nbsp;</p><p><span>计算 L(λ) 的梯度如下：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTFyuCUjkYLKbeqL1WVhborXrZ9lOZycQCHAULLb03qgPfbx4CjQmHIQ/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>公式（41）倒数第三行到导数第二行的推导利用了 score function 的期望为 0 这一性质，基于公式（41）就可以利用蒙特卡洛采样进行梯度估计，然后利用随机优化算法进行参数的更新。算法流程图如下：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uThBaj1cnjCvkKrZDUxVaU2TqeO22DGzNDdJByLzIZquPnXWsEe41x6w/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>参数估计除了要保证无偏之外，还希望估计的方差要尽量小。在此基础上，本节介绍一种经典的降低方差的方法 Control Variates，也会用到 score function 的一些性质。</span></p><p>&nbsp;</p><p><span>这里，假设一个估计是 f，希望可以找到一个新估计</span><img src='https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgm432OuArXS3GoqcRs7O7XKBrDAQrqDaB159KiaKL6xI2KFtPeqAXaicRRbQWQRZBib86sJBEpIicCiblQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='img' referrerPolicy='no-referrer' /><span>，使得：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTJxoNfaz39qDanhqB920QUp4RCTIicITHrwZsG9MtuKicQfkPrhflKg5w/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>控制变量法是构造一类估计函数，定义如下：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uT1QcpqR3FnzLFxh25J2yCjz7xNHPNrLsKt34tTq35frqBY0MOe9iarxQ/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>其中，a 是一个标量，h 是一个函数。由公式（43）容易得到，</span><img src='https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgm432OuArXS3GoqcRs7O7XKBrDAQrqDaB159KiaKL6xI2KFtPeqAXaicRRbQWQRZBib86sJBEpIicCiblQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='img' referrerPolicy='no-referrer' /><span> 和 f 的期望相同，方差如下：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTiamAye8zbov3tlGLlfSRYIVHMeKpxUKX3RGNI27D7M3l9Zia2Sz2SI5w/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>直观上讲，Cov(f,h) 越大，新估计的方差越小，控制变量效果越好。令：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTVkcoCnF1UicGupX5cZnDrrpfTBM8BlIN7EEWcHzP59Tm7BjzwAdZCkw/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>可得：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uT3icbPMB24c9ulIV0WOKx47jic9B0Kw7jzLrg46ZL0OOo9AqhwSIsbOAA/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>最优参数值是协方差和方差之比。为了方便计算，函数 h(z) 的选择是 score function，即：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTJJa5gaf6RtGPXFXib49Py5Fj02AMej1cC8MPrfVTfXUH7YD59XESyew/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>所以，E[h(z)]=0。</span></p><p>&nbsp;</p><p><span>用新的估计</span><img src='https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgm432OuArXS3GoqcRs7O7XKBrDAQrqDaB159KiaKL6xI2KFtPeqAXaicRRbQWQRZBib86sJBEpIicCiblQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='img' referrerPolicy='no-referrer' /><span>来替换公式（41）中的估计 f，如下：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTdKUkxHIt7ZhVnVBheZ6S7Xic39aJCx9ew5lJ4URKibDxt9QaHztF5Z0Q/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>基于蒙特卡洛采样对梯度进行估计，从上述推导中可以保证新的估计方差会更小。</span></p><p>&nbsp;</p><p><span>Score Function 在使用时一般要满足以下条件：</span></p><p>&nbsp;</p><ul><li><span>代价函数 f(x) 可以是任意函数。比如可微的，不可微的；离散的，连续的；白箱的，黑箱的等。</span></li><li><span>这个性质是其最大的优点，使得很多不可微的甚至没有具体函数的黑箱优化问题都可以利用梯度优化求解。</span></li><li><span>分布函数 p(x;θ) 必须对 θ 是可微的，从公式中也看得出来。</span></li><li><span>分布函数必须是便于采样的，因为梯度估计都是基于 MC 的，所以希望分布函数便于采样。</span></li><li><span>SF 的方差受很多因素影响，包括输入的维度和代价函数。</span></li></ul><p>&nbsp;</p><p><span>另外，SF 还有一些其他的名称，Likelihood Ratio，Automated Variational Inference，REINFORCE，Policy Gradients，在机器学习的很多领域中都有广泛的应用。</span></p><p>&nbsp;</p><p><strong><span>Pathwise Derivative (PD)</span></strong></p><p>&nbsp;</p><p><span>不同于 Score Function 对代价函数没有任何约束，PD 要求代价函数可微，虽然 SF 更具一般性，但 PD 会有更好的性质。PD 在机器学习领域有另一个名称是 reparameterization trick，它是著名的深度生成模型 VAE 中一个重要的步骤。</span></p><p>&nbsp;</p><p><span>PD 的思路是将待学习的参数从分布中变换到代价函数中，核心是做分布变换（即所谓的 reparameterization，重参数化），计算原来分布下的期望梯度时，由于变换后的分布不包含求导参数，可将求导和积分操作进行对换，从而基于 MC 对梯度进行估计。</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTxdcGazS1QyQ5XxwhgwJRl7fg7pSwicsGMQdo6p8nmK5jmRorKqsWKHg/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>如上述公式，从一个含参 θ 分布中采样，等同于从一个简单无参分布中采样，然后进行函数变换，并且此函数的参数也是 θ。变换前，采样是直接从所给分布中进行，而采用重参数化技巧后，采样是间接地从一个简单分布进行，然后再映射回去，这个映射是一个确定性的映射。其中，映射有很多中思路，比如：逆函数、极变换等方法。</span></p><p>&nbsp;</p><p><span>PD 的一个重要理论依据是 Law of the Unconscious Statistician (LOTUS) ，即：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uT8zteKvBibmjSKpqTMAldhpLRaQjltZBjEuic926CFCJb4obBtnVZvSTQ/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>从定理中可以看到，计算一个函数的期望，可以不知道其分布，只需要知道一个简单分布，以及从简单分布到当前分布的映射关系即可。</span></p><p>&nbsp;</p><p><span>基于 Law of the Unconscious Statistician (LOTUS) 对 PD 进行推导，如下：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTxRqQaKrvATDhpfr4iaQUJTeQjPcsFaOib86tAuePlE1FKPKtQbVBKKVg/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>利用 MC 可以估计出梯度为：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTZfmHJhwJmJwqM71icxFXItIVxaO8sV7ib9UbdFsk9RK6OWjQhMuzkbIw/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>其中 </span><img src='https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uT4Jy6B1qxrYWtaoUGHGsCs9nIj8sVibz0QicdjtNtxkKj7fTbZ4GeXqBA/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='img' referrerPolicy='no-referrer' /><span>。从推导中可以看出，分布中的参数 θ 被 push 到了代价函数中，从而可以将求导和积分操作进行对换。</span></p><p>&nbsp;</p><p><span>分布变换是统计学中一个基本的操作，在计算机中实际产生各种常见分布的随机数时，都是基于均匀分布的变换来完成的。有一些常见的分布变换可参见下表：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTR560khUrNnKo2HUtns4xwJdgUJkqTE6kTOmbcvZW6dI5fH9CVnicLGg/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p><span>▲ 图：常见分布变换</span></p><p>&nbsp;</p><p><span>在使用 PD 时需要满足以下性质：</span></p><p>&nbsp;</p><ul><li><p><span>代价函数要求是可微的，比 SF 更严格</span></p></li><li><p><span>在使用 PD 时，并不需要显式知道分布的形式，只需要知道一个基础分布和从该基础分布到原分布的一个映射关系即可，这意味着，不管原来分布多么复杂，只要能获取到以上两点信息，都可以进行梯度估计；而 SF 则需要尽量选择一个易采样的分布</span></p></li><li><p><span>PD 的方差受代价函数的光滑性影响</span></p><p>&nbsp;</p></li></ul><p><span>另外，PD 还有一些其他名称，Stochastic backpropagation，Affiffiffine-independent inference 和 Reparameterisation Tricks 等。</span></p><p>&nbsp;</p><h1><a name="%E5%BA%94%E7%94%A8" class="md-header-anchor"></a><span>应用</span></h1><p>&nbsp;</p><p><span>变分推断方法在深度学习中有两个非常典型而且热门的应用，一个是贝叶斯神经网络 BNN，一个是变分自编码器 VAE。后续会专门写两篇关于 BNN 和 VAE 的综述，这里简单介绍一下。</span></p><p>&nbsp;</p><p><strong><span>贝叶斯神经网络</span></strong><span> </span><strong><span>BNN</span></strong></p><p>&nbsp;</p><p><span>贝叶斯神经网络不同于一般的神经网络，其权重参数是随机变量，而非确定的值。如下图所示：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTUnG9vOKaibrxWdtWBOLRrF0AcXpeVWGSLvC99ZelG6ZCBmsPpJ9obow/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>假设 NN 的网络参数为 W，p(W) 是参数的先验分布，给定观测数据 D={X,Y}，这里 X 是输入数据，Y 是标签数据。BNN 希望给出以下的分布：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTsrxqkXJyGO2xfZZMpra1FWq0XIZ3fluKLrOAiaWuj2E0hWibMf8NZY3g/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>其中：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTibzd0N0WoeMH842WpbxgtsqTiaSTDqfsKy7cb4ljQiagibYlExTTrO6v6A/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>这里 P(W|D) 是后验分布，P(D|W) 是似然函数，P(D) 是边缘似然。</span></p><p>&nbsp;</p><p><span>从公式（53）中可以看出，用 BNN 对数据进行概率建模并预测的核心在于做高效近似后验推断，而 VI 是一个非常合适的方法。</span></p><p>&nbsp;</p><p><span>BNN 不同于 DNN，可以对预测分布进行学习，不仅可以给出预测值，而且可以给出预测的不确定性。这对于很多问题来说非常关键，比如：机器学习中著名的 Exploration &amp; Exploitation （EE）的问题，在强化学习问题中，agent 是需要利用现有知识来做决策还是尝试一些未知的东西；实验设计问题中，用贝叶斯优化来调超参数，选择下一个点是根据当前模型的最优值还是利用探索一些不确定性较高的空间。比如：异常样本检测，对抗样本检测等任务，由于 BNN 具有不确定性量化能力，所以具有非常强的鲁棒性。</span></p><p>&nbsp;</p><p><strong><span>变分自编码器</span></strong><span> </span><strong><span>VAE</span></strong></p><p>&nbsp;</p><p><span>深度生成模型中两个最有名的模型是 GAN 和 VAE，有工作介绍过GAN 和 VAE 从 VI 的视角是可以统一起来的。这类简单介绍一下VAE，VAE 是一种隐变量模型（Latent Variable Model, LVM）和深度学习巧妙结合的产物。如下图：</span></p><p>&nbsp;</p><p><img src='https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgkia8dWrGUhRoj0cuGvS53uTOOicJjt8OInl3YzpciaicRhEWftCHsDpDjtMztKoHtSl4ffQ0fDniaxpsw/640?tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1' alt='image.png' referrerPolicy='no-referrer' /></p><p>&nbsp;</p><p><span>模型中由两个部分构成，一个是观测数据 x 到隐变量 z 的映射模型，称为 encoder，另一个是从隐变量 z 到观测数据 x 的映射模型，称为 decoder。encoder 相当于是求一个后验分布，这里用 VI 的方法，假设用一个高斯变分分布 qф(z|x) 来代替真实的后验分布，并用 DNN 来近似逼近这个高斯分布的均值和方差。</span></p><p>&nbsp;</p><p><span>从上图中可以看到，经过encoder 和decoder 的映射，输入是观测数据 x，期待的输出数据也是 x，不需要对样本进行标注。VAE 在 loss function 推导时需要用到随机梯度估计一节提到的 Pathwise Derivative，在这里叫做 Reparameterization Trick 来估计梯度。VAE 是一个非常有趣的模型，从结构上来看，隐变量空间相当于是一个原数据的降维表示，在很多地方将会有非常有趣的应用。</span></p><p>&nbsp;</p><p><span>VI 除了应用在 BNN 和 VAE 之外，还在 Policy Gradient、AutoML 和 PGM 等领域被广泛使用。</span></p><p>&nbsp;</p><h1><a name="%E6%80%BB%E7%BB%93" class="md-header-anchor"></a><span>总结</span></h1><p>&nbsp;</p><p><span>本文是对 VI 方法进展的一个简述，主要思路是从 VI 经典方法和相关的几个问题来展开，包括：分布测度、复杂先验、复杂后验、VI 的可扩展性以及应用来叙述。</span></p><p>&nbsp;</p><p><span>由于 VI 是一大类方法，每年新增的研究工作不计其数，近二年与 MCMC 的结合和统一催生了一批新的高效方法和理论分析，实在难以覆盖所有工作。希望通过本文介绍的思路，读者可以快速地了解这个领域，做应用的同学可以找一些合适的方法来解决应用问题，准备做机器学习理论研究的同学可以参考本文的思路，有针对性地寻找到感兴趣的方向。 </span></p><p>&nbsp;</p><h1><a name="%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE" class="md-header-anchor"></a><span>参考文献</span></h1><p>&nbsp;</p><p><span>[1] Zhang, Cheng, et al. “Advances in Variational Inference.” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 8, 2019, pp. 2008–2026.</span></p><p><span>[2] Blei, David M., et al. “Variational Inference: A Review for Statisticians.” Journal of the American Statistical Association, vol. 112, no. 518, 2017, pp. 859–877.</span></p><p><span>[3] Wainwright, Martin J., and Michael I. Jordan. Graphical Models, Exponential Families, and Variational Inference. 2008.</span></p><p><span>[4] Rezende, Danilo Jimenez, and Shakir Mohamed. “Variational Inference with Normalizing Flows.” ArXiv Preprint ArXiv:1505.05770, 2015.</span></p><p><span>[5] Kingma, Diederik P., et al. “Improving Variational Inference with Inverse Autoregressive Flow.” ArXiv Preprint ArXiv:1606.04934, 2016.</span></p><p><span>[6] Tran, Dustin, et al. “Copula Variational Inference.” NIPS’15 Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2, 2015, pp. 3564–3572.</span></p><p><span>[7] Ranganath, Rajesh, et al. “Operator Variational Inference.” NIPS’16 Proceedings of the 30th International Conference on Neural Information Processing Systems, 2016, pp. 496–504.</span></p><p><span>[8] Liu, Qiang, and Dilin Wang. “Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm.” Advances in Neural Information Processing Systems, 2016, pp. 2370–2378.</span></p><p><span>[9] Atanov, Andrei, et al. “The Deep Weight Prior.” ICLR 2019 : 7th International Conference on Learning Representations, 2019.</span></p><p><span>[10] Tomczak, Jakub M., and Max Welling. “VAE with a VampPrior.” ArXiv Preprint ArXiv:1705.07120, 2017.</span></p><p><span>[11] Yin, Mingzhang, and Mingyuan Zhou. “Semi-Implicit Variational Inference.” ICML 2018: Thirty Fifth International Conference on Machine Learning, 2018, pp. 5646–5655.</span></p><p><span>[12] Ranganath, Rajesh, et al. “Black Box Variational Inference.” Journal of Machine Learning Research, vol. 33, 2014, pp. 814–822.</span></p></div>
</body>
</html>