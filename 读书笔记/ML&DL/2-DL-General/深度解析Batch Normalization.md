# 深度解析Batch Normalization

#### **前言**

这是 2015 年深度学习领域非常棒的一篇文献：《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》，这个算法目前已经被大量的应用，最新的文献算法很多都会引用这个算法，进行网络训练。

##### BN 的优点是： 

- 可选择较大初始学习率，**加快收敛**。
    - 使用小学习率，收敛速度也会很快
- 减少正则化参数的 Dropout、L2 正则项参数的选择问题
    - **BN 具有提高网络泛化能力的特性** 
- 不需要使用局部响应归一化层
    - BN 本身就是一个**归一化网络层**；
- **可以把训练数据彻底打乱** 
    - 防止每批训练的时候，某一个样本经常被挑选到，在 ImageNet 上提高 1% 的精度。 

**BN 的核心思想**不是为了防止梯度消失或者防止过拟合，其核心是通过对系统参数搜索空间进行约束来增加系统鲁棒性，这种约束压缩了搜索空间，约束也改善了系统的结构合理性，这会带来一系列的性能改善，比如加速收敛，保证梯度，缓解过拟合等。

#### **Why Normalization** 

机器学习领域有个很重要的假设：**IID 独立同分布假设**，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。

因此，在把数据喂给机器学习模型之前，“白化（whitening）”是一个重要的数据预处理步骤，其中最典型白化方法是 PCA。白化一般包含两个目的： 

- 去除特征间的相关性：独立； 
- 使所有特征有相同均值和方差 ：同分布。 

每批训练数据的分布各不相同，那么网络需要在每次迭代中去学习适应不同的分布，这样将会大大降低网络的训练速度。

对于深度网络的训练是一个非常复杂的过程，只要网络的前面几层发生微小的改变，那么这些微小的改变在后面的层就会被累积放大下去。

一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，训练数据的分布一直在发生变化，那么将会影响网络的训练速度。

#### **什么是Internal Covariate Shift（内部协变量转移）** 

在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布是一致的”。

如果不一致，那么就出现了新的机器学习问题，如 transfer learning / domain adaptation 等。

而 covariate shift 就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同，即：对所有 $x\in X$,  $P_s(Y|X=x)=P_t(Y|X=x)$. 但是, $P_s(X)\neq P_t(X)$. 

大家细想便会发现，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了 covariate shift 的定义。由于是对层间信号的分析，也即是“internal”的来由。 

简单点说就是：对于深度学习这种包含很多隐层的网络结构，在训练过程中，因为各层参数不停在变化，所以每个隐层都会面临 covariate shift 的问题，也就是**在训练过程中，隐层的输入分布老是变来变去，这就是所谓的“Internal Covariate Shift”，Internal 指的是深层网络的隐层，是发生在网络内部的事情，而不是 covariate shift 问题只发生在输入层。** 

#### ICS 的问题

- 每个神经元的输入数据不再是“独立同分布”；
- 上层参数需要不断适应新的输入数据分布，降低学习速度；
- 下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止；
- 每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。

#### BN的设计思路

在每一层输入的时候，再加个预处理操作那该有多好啊，比如网络第三层输入数据把它归一化至：均值 0、方差为 1，然后再输入第三层计算，这样我们就可以解决前面所提到的“Internal Covariate Shift”的问题了。 

Paper 的想法就是：在网络的每一层输入的时候，又插入了一个归一化层，也就是先做一个归一化处理，然后再进入网络的下一层。 

说到神经网络输入数据预处理，最好的算法莫过于白化预处理。然而白化计算量太大了，很不划算，还有就是白化不是处处可微的，所以在深度学习中，其实很少用到白化。经过白化预处理后，数据满足以下两个条件： 

- 去除特征之间的相关性：独立； 
- 使得所有特征具有相同的均值和方差 ：同分布。 

但是实现白化的计算量非常大，于是为了简化计算，作者忽略了第 1 个要求，仅仅使用了近似白化预处理.

然而实现起来并不是那么简单的。如果仅仅使用上面的归一化公式，对网络某一层 A 的输出数据做归一化，然后送入网络下一层 B，这样是会影响到本层网络 A 所学习到的特征的。

> 打个比方，比如我网络中间某一层学习到特征数据本身就分布在 S 型激活函数的两侧，你强制把它给我归一化处理、标准差也限制在了 1，把数据变换成分布于 s 函数的中间部分，这样就相当于我这一层网络所学习到的特征分布被你搞坏了，

这可怎么办？于是文献使出了一招惊天地泣鬼神的招式：变换重构，引入了可学习参数 γ、β，这就是算法关键之处：
$$
y_i 
<- \gamma \overline{x}_i+\beta
$$
