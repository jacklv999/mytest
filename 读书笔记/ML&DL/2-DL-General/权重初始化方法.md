## 权重初始化方法

#### 1. Xavier 初始化

对关于给定值的对称激活函数, 如双曲正切函数和 sigmoid 函数等, Glorot和Bengio证明，使用Xavier初始化的网络会有更快的收敛速度和更高的准确性。

#### 2. Kaming 初始化

探索如何用类relu的激活函数在网络中最好地初始化权重是kobjective He等人，提出他们自己的初始化方案的动机，这是为使用这些非对称、非线性激活的深层神经网络量身定制的。

He et. al.在他们2015年的论文中证明，如果使用以下输入权初始化策略，深度网络(例如22层CNN)将会更早地收敛：

1. 为给定层上的权值矩阵创建一个张量，并用从标准正态分布中随机选择的数字填充它。
2. 将每个随机选择的数字乘以*√*2/*√n*，其中*n*是从上一层的输出(也称为“扇入”)进入给定层的连接数。
3. 偏置张量初始化为零。

我们可以按照这些方向实现我们自己版本的kming初始化，并验证如果在我们假设的100层网络的所有层上使用ReLU，那么它确实可以防止激活输出爆炸或消失。