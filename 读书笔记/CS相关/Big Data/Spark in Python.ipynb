{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the notes for pyspark, it contains the basic operation for pyspark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The installation is a little complicate for rookie. We need to follow steps:\n",
    "- installing java(jdk8 was preferred, dueing to a known issue of jdk11);\n",
    "- installing hadoop:\n",
    "  - downloading hadoop files\n",
    "  - configuring hadoop, hdfs, yarn;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "## create SparkContexts\n",
    "## method 1\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "## method 2\n",
    "#from pyspark import SparkContext, SparkConf\n",
    "#conf = SparkConf().setAppName(\"local\").setMaster(\"master\")\n",
    "#sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data from array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two data format for Spark: RDD and dataframe. \n",
    "RDD was designed for:\n",
    "- low-level transformation and actions\n",
    "- unstructured data, such as media streams or streams of text;\n",
    "- manipulating data with functional programming ;\n",
    "\n",
    "Else, you should use dataframe.\n",
    "\n",
    "Note: dataset were merged to dataframe in spark2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(appName).setMaster(master)\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "data = [1, 2, 3, 4, 5]\n",
    "RDD_data_1 = sc.parallelize(data)\n",
    "RDD_data_2 = sc.textFile(\"data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: bigint, c: double]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "tmp = spark.createDataFrame([\n",
    "    Row(a=1, b=3, c=1.0),\n",
    "    Row(a=1, b=3, c=2.0),\n",
    "    Row(a=1, b=4, c=3.0),\n",
    "    Row(a=1, b=4, c=4.0)\n",
    "], schema = ['a','b','c'])\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2   3   4   5\n",
       "a  1  2  3   4   5   6\n",
       "b  7  8  9  10  11  12"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "tmp_1 = ps.DataFrame({'a': [1,2,3,4,5,6], 'b':[7,8,9,10,11,12]})\n",
    "tmp_1.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference between two dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas on Spark overcomes the data limitation, enabling users to work with large datasets by leveraging Spark.\n",
    "\n",
    "Both multi-threading and Spark SQL Catalyst Optimizer contribute to the optimized performance of pandas on spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Users can directly query data via SQL with Spark’s optimized SQL engine, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "pdf = pd.DataFrame({\"a\": [1, 3, 5]})  # pandas DataFrame\n",
    "sdf = spark.createDataFrame(pdf)  # PySpark DataFrame\n",
    "psdf = sdf.to_pandas_on_spark()  # pandas-on-Spark DataFrame\n",
    "\n",
    "# Query via SQL\n",
    "ps.sql(\"SELECT count(*) as num FROM {psdf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It also supports string interpolation syntax to interact with Python objects naturally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = range(4)\n",
    "\n",
    "# String interpolation with Python instances\n",
    "ps.sql(\"SELECT * from {psdf} WHERE a IN {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----------+--------+-----+-----+-------------+\n",
      "|adgroup_id|cate_id|campaign_id|customer|brand|price|numeric_price|\n",
      "+----------+-------+-----------+--------+-----+-----+-------------+\n",
      "|     63133|   6406|      83237|       1|95471|170.0|          170|\n",
      "|    313401|   6406|      83237|       1|87331|199.0|          199|\n",
      "|    248909|    392|      83237|       1|32233| 38.0|           38|\n",
      "+----------+-------+-----------+--------+-----+-----+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\n",
    "    'file:///home/test/Desktop/Datasets/Ad DisplayClick Data on Taobao.com/data/ad_feature.csv', \n",
    "    header=True)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show first $n$ line of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(adgroup_id='63133', cate_id='6406', campaign_id='83237', customer='1', brand='95471', price='170.0')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show last $n$ line of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(adgroup_id='845337', cate_id='11156', campaign_id='379603', customer='255874', brand='74120', price='279.0')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adgroup_id', 'cate_id', 'campaign_id', 'customer', 'brand', 'price']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe specific column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|       adgroup_id|\n",
      "+-------+-----------------+\n",
      "|  count|           846811|\n",
      "|   mean|         423406.0|\n",
      "| stddev|244453.4237388931|\n",
      "|    min|                1|\n",
      "|    max|            99999|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "df.select(\"adgroup_id\").describe().show()\n",
    "\n",
    "# method 2\n",
    "user_cart_faved.describe(\"user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(how='any', thresh=None).describe().show()\n",
    "\n",
    "# how : str, optional, 'any' or 'all';\n",
    "# thresh: int, optional, If specified, drop rows that have less than `thresh` non-null values. \n",
    "#         This overwrites the `how` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method dropDuplicates in module pyspark.sql.dataframe:\n",
      "\n",
      "dropDuplicates(subset=None) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.drop_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "help(df.fillna)\n",
    "\n",
    "fillna(value, subset=None) method of pyspark.sql.dataframe.DataFrame instance\n",
    "    Replace null values, alias for ``na.fill()``.\n",
    "    \n",
    "    :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
    "    \n",
    "    .. versionadded:: 1.3.1\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    value : int, float, string, bool or dict\n",
    "        Value to replace null values with.\n",
    "        If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
    "        from column name (string) to replacement value. The replacement value must be\n",
    "        an int, float, boolean, or string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv('foo.csv', header=True)\n",
    "df.write.parquet('bar.parquet')\n",
    "df.write.orc('zoo.orc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data slice and index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame.select() takes the Column instances that returns another DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"price\"]  # more useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[adgroup_id: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"adgroup_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('adgroup_id', 'string')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"adgroup_id\").dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|data type|range|\n",
    "| --- | --- |\n",
    "|Byte|\tAn integer representing a byte. The range is -128 to 127|\n",
    "|Short\t|An integer representing two bytes. The range is -32768 to 32767|\n",
    "|Int\t|An integer representing 4 bytes. The range is -2147483648 to 2147483647|\n",
    "|Long\t|An integer representing 8 bytes. The range is -9223372036854775808 to 9223372036854775807|\n",
    "|Float|\trepresents a 4-byte single-precision floating-point number|\n",
    "|Double|\trepresents an 8-byte double-precision floating-point number|\n",
    "|String\t|represents a string value|\n",
    "|Boolean\t|represents the boolean value|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('customer', 'int')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType,BooleanType,DateType\n",
    "\n",
    "res_col = user_behavior[\"brand\"].cast(\"int\")\n",
    "user_behavior = user_behavior.withColumn(\"fav_brand\", col = res_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"numeric_price\", df.price.cast(\"long\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----------+--------+------+-----+\n",
      "|adgroup_id|cate_id|campaign_id|customer| brand|price|\n",
      "+----------+-------+-----------+--------+------+-----+\n",
      "|    138953|  11156|     137119|     173|183367| 26.0|\n",
      "|    467512|  11156|     137119|     173|183367| 49.9|\n",
      "|    140008|  11156|     395908|    1214| 10677|138.0|\n",
      "|    238772|  11156|     385820|    1214| 10677|  6.0|\n",
      "|    237471|  11156|     385820|    1214| 58577|148.0|\n",
      "+----------+-------+-----------+--------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.cate_id == 11156).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using built-in functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 144:>                                                        (0 + 8) / 8]\r",
      "\r",
      "[Stage 144:=======>                                                 (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+\n",
      "|cate_id|max(price)|        avg(brand)|\n",
      "+-------+----------+------------------+\n",
      "|      1|    9999.0| 68535.29129662522|\n",
      "|     10|     999.0|164078.31118881117|\n",
      "|   1000|       9.9|138492.57142857142|\n",
      "|  10000|     180.0|          374851.0|\n",
      "|  10001|     900.0|165290.77777777778|\n",
      "+-------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupby(\"cate_id\").agg({\"brand\":\"mean\",\"price\":\"max\"}).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+\n",
      "|cate_id| brand|avg(numeric_price)|\n",
      "+-------+------+------------------+\n",
      "|   7214|412664|              76.0|\n",
      "|  11292| 61561|             700.0|\n",
      "|   7214|105646|              48.5|\n",
      "|   6806|132353|175.46666666666667|\n",
      "|   6426|222775|             743.5|\n",
      "+-------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_mean_price = df.groupby(\"cate_id\",\"brand\").avg(\"numeric_price\")\n",
    "df_with_mean_price.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using user defined functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It only supports operation by row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **pandas udf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|multiply_func(a, b)|\n",
      "+-------------------+\n",
      "|                  3|\n",
      "+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf('long')\n",
    "def multiply_func(a: pd.Series, b: pd.Series) -> pd.Series:\n",
    "    return a * b\n",
    "\n",
    "# Execute function as a Spark vectorized UDF\n",
    "tmp.select(multiply_func(tmp.a, tmp.b)).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **applyInPandas**: \n",
    "\n",
    "    The function should take a `pandas.DataFrame` and return another `pandas.DataFrame`. \n",
    "    \n",
    "    For each group, all columns are passed together as a `pandas.DataFrame` to the user-function and the returned `pandas.DataFrame` are combined as a class:`DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  3|1.5|\n",
      "|  1|  3|1.5|\n",
      "+---+---+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "def plus_mean(pandas_df):\n",
    "    return pandas_df.assign(c=pandas_df.c.mean())\n",
    "\n",
    "tmp.groupby('a','b').applyInPandas(plus_mean, schema=tmp.schema).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **mapInPandas**: \n",
    "\n",
    "    The function should take an iterator of `pandas.DataFrame` and return another iterator of `pandas.DataFrame`. \n",
    "    \n",
    "    All columns are passed together as an iterator of `pandas.DataFrame` to the function and the returned iterator of `pandas.DataFrame` are combined as a :class:`DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  3|2.0|\n",
      "|  1|  3|3.0|\n",
      "|  1|  4|4.0|\n",
      "|  1|  4|5.0|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "def filter_func(iterator):\n",
    "    for pdf in iterator:\n",
    "        pdf.c = pdf.c + 1\n",
    "        yield pdf[pdf.a == 1]\n",
    "\n",
    "tmp.mapInPandas(filter_func, tmp.schema).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **transform**: must return data with same shape\n",
    "- **transfrom in batch**: slice the pandas-on-Spark DataFrame or Series, and then applies the given function with pandas DataFrame or Series as input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1   2   3   4   5\n",
       "a  2  3   4   5   6   7\n",
       "b  8  9  10  11  12  13"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pandas_plus(pser):\n",
    "    return pser + 1  \n",
    "\n",
    "tmp_1.transform(pandas_plus).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/31 00:34:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "21/12/31 00:34:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7\n",
       "a  1.0  1.0  1.0  1.0  2.0  2.0  2.0  2.0\n",
       "b  3.0  3.0  4.0  4.0  5.0  5.0  6.0  6.0\n",
       "c  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pandas_plus(pser):\n",
    "    if pser.a[0] == 1:\n",
    "        pser.c = pser.c + 1\n",
    "    else:\n",
    "        pser.c = pser.c - 1\n",
    "    return pser\n",
    "\n",
    "tmp.pandas_on_spark.transform_batch(pandas_plus).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **apply**: it can return data in any shape\n",
    "- **apply_batch**: slice the pandas-on-Spark DataFrame or Series, and then applies the given function with pandas DataFrame or Series as input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    21\n",
       "b    57\n",
       "dtype: int64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pandas_plus(pser):\n",
    "    return sum(pser)\n",
    "\n",
    "tmp_1.apply(pandas_plus, axis=0)\n",
    "tmp_1.pandas_on_spark.apply_batch(pandas_plus).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  846811|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"tableA\")\n",
    "spark.sql(\"SELECT count(*) from tableA\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, UDFs can be registered and invoked in SQL out of the box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"integer\")\n",
    "def add_one(s: pd.Series) -> pd.Series:\n",
    "    return s + 1\n",
    "\n",
    "spark.udf.register(\"add_one\", add_one)\n",
    "spark.sql(\"SELECT add_one(v1) FROM tableA\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas on Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has full support on pandas API and optimizied on SQL engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: User Profile Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a demo for user profile from the Taobao AD data. We are going to build user profile step by step. And the main user profile we going to build including user value, loyalty and interests.\n",
    "\n",
    "The user value and loyalty were building by: \n",
    "- High/Low Value: Likes buying items which price higher than $P_{avg} + \\sigma_{p}$;\n",
    "- Loyalty/Common User: Frequency more than $Trans_{avg}+\\sigma_{trans}$;\n",
    "\n",
    "\n",
    "The user interests were building by:\n",
    "\n",
    "- Modeling Labels\n",
    "    - User interests: Current likes/Long-term likes/Do not likes\n",
    "    - Interest weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用户的行为日志behavior_log：用户ID，行为类型，时间，商品类目ID，品牌ID\n",
    "\n",
    "本数据集涵盖了raw_sample中全部用户22天内的购物行为(共七亿条记录)。字段说明如下：\n",
    "- (1) user：脱敏过的用户ID；\n",
    "- (2) time_stamp：时间戳；\n",
    "- (3) btag：行为类型,  包括以下四种：浏览(pv), 喜欢(fav), 加入购物车(cart), 购买(buy)\n",
    "- (4) cate：脱敏过的商品类目；\n",
    "- (5) brand: 脱敏过的品牌词；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing cate/brand avg price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+\n",
      "|cate_id| brand|    avg(num_price)|\n",
      "+-------+------+------------------+\n",
      "|   7214|412664|              76.0|\n",
      "|  11292| 61561|             700.0|\n",
      "|   7214|105646|              48.5|\n",
      "|   6806|132353|175.46666666666667|\n",
      "|   6426|222775|             743.5|\n",
      "+-------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ad_feature = spark.read.csv(\n",
    "    'file:///home/test/Desktop/Datasets/Ad DisplayClick Data on Taobao.com/data/ad_feature.csv', \n",
    "    header=True)\n",
    "\n",
    "res_col = ad_feature[\"price\"].cast(\"float\")\n",
    "ad_feature = ad_feature.withColumn(\"num_price\", col = res_col)\n",
    "ad_mean = ad_feature.groupby(\"cate_id\",\"brand\").avg(\"num_price\")\n",
    "ad_mean.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing user pvalue_level(消费档次) by brand fav and cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+---------+\n",
      "|   user|cate| brand|fav_brand|\n",
      "+-------+----+------+---------+\n",
      "|    100|6423|326445|        1|\n",
      "|1000000|6554|275122|        1|\n",
      "|1000000|6952|265474|        2|\n",
      "|1000000|6952|265474|        2|\n",
      "|1000000|6952|348810|        1|\n",
      "|1000000|6952|275122|        2|\n",
      "|1000000|6952|275122|        2|\n",
      "|1000000|6952| 59455|        2|\n",
      "|1000000|6952| 59455|        2|\n",
      "|1000000|7214|275077|        1|\n",
      "+-------+----+------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "user_behavior = spark.read.csv(\n",
    "    'file:///home/test/Desktop/Datasets/Ad DisplayClick Data on Taobao.com/data/behavior_log.csv', \n",
    "    header=True)\n",
    "\n",
    "res_col = user_behavior[\"brand\"].cast(\"int\")\n",
    "user_behavior = user_behavior.withColumn(\"fav_brand\", col = res_col)\n",
    "user_cart = user_behavior.filter(user_behavior.btag == \"cart\")\n",
    "user_cart = user_cart.select(\"user\",\"cate\", \"brand\",\"fav_brand\")\n",
    "\n",
    "## on the specific cate, finding user most carted brand\n",
    "def compute_freq(pdf):\n",
    "    pdf_tmp = pd.Series(pdf.brand)\n",
    "    CountDict = dict(pdf_tmp.value_counts())\n",
    "    for i in CountDict.keys():\n",
    "        tmp_index = pdf.brand == i\n",
    "        pdf.loc[tmp_index,'fav_brand'] = CountDict[i]\n",
    "    return pdf\n",
    "\n",
    "#tmp_3 = tmp.groupby(\"user\",\"cate\").applyInPandas(compute_freq, schema=tmp.schema)\n",
    "user_cart_faved = user_cart.groupby(\"user\",\"cate\").applyInPandas(compute_freq, schema=user_cart.schema)\n",
    "user_cart_faved = user_cart_faved.dropDuplicates()\n",
    "user_cart_faved.show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
